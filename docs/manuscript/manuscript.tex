\documentclass[]{interact}
\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.
\usepackage{subfigure}% Support for small, `sub' figures and tables
%\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required

\usepackage{natbib}
\bibliographystyle{chicago}
\setcitestyle{authoryear,open={(},close={)}}
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}% Bibliography support using natbib.sty

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=blue,
}

\usepackage{titlesec}
\titleformat*{\section}{\Large\bfseries}
\titleformat*{\subsection}{\large\bfseries}

\usepackage{endnotes}
\let\footnote=\endnote
\usepackage{etoolbox}
\patchcmd{\enoteformat}{1.8em}{0pt}{}{}

\theoremstyle{plain}% Theorem-like structures provided by amsthm.sty
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}

\usepackage{tabularx}
\usepackage{booktabs,caption}
\usepackage{threeparttable}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\articletype{DRAFT MANUSCRIPT}

\title{Reproducible Research Practices and Barriers to Reproducible Research in Geography: Insights from a Survey}

\author{
\name{Peter Kedron\textsuperscript{a,b}\thanks{CONTACT Peter Kedron. Email: Peter.Kedron@asu.edu}, Joseph Holler\textsuperscript{c}, and Sarah Bardin\textsuperscript{a,b}}
\affil{\textsuperscript{a}School of Geographical Sciences and Urban Planning, Arizona State University, Tempe, Arizona, USA; \textsuperscript{b}Spatial Analysis Research Center (SPARC), Arizona State University, Tempe, Arizona, USA; \textsuperscript{c}Department of Geography, Middlebury College, Middlebury, Vermont, USA}
}

\maketitle

\begin{abstract}
While the number of reproduction and replication studies undertaken in the social and behavioural sciences continues to rise, such studies have not yet become commonplace in geography. 
Existing attempts to reproduce geographic research suggest that many studies cannot be fully reproduced, or are simply missing components needed to attempt a reproduction. 
Despite this suggestive evidence, we have not yet systematically assessed geographers' perceptions of reproducibility, the use of reproducible research practices across the discipline's diverse research traditions, or identified the factors that have kept geographers from conducting more reproduction studies.
This study addresses each of these questions by surveying active geographic researchers selected using probability sampling techniques from a rigorously constructed sampling frame.
We find ...
CONClUSION...

\end{abstract}

\begin{keywords}
Reproducible Research, Epistemology, Geographic Research Methods
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Introduction}
Reproducibility as a characteristic of a study that lends it credibility v. Reproducibility as a error correcting function of science. 

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Assessing the Reproducibility of Research}
The reproducibility of research can be assessed directly or indirectly. 
In a direct assessment of reproducibility, an independent team(s) of researchers repeats the analytical steps of a previous study using the same data and then evaluates the consistency of the two sets of results\footnote{Insert paragraph on the difficulty of developing in using consistency criteria}.
By contrast, during an indirect assessment of reproducibility, a research team gathers information about the transparency of a study and the availability of the research components (e.g., data and code) needed to recreate that study, but does not directly attempt to recreate the results. 
Indirect assessments also often take the form of surveys in which researchers are asked about their research practices, incentives, and motivations. 

Direct and indirect reproducibility assessments present a trade off between the depth and breadth of evaluation.  
A direct assessment that carefully reproduces a study can provide insight into how a particular study was conducted and why a consistent result could, or could not, be obtained.  
However, such assessments are resource and time intensive, which makes them difficult to scale to a large number of studies. 
Constrained to a limited number of studies, direct assessments offer only suggestive evidence about the reproducibility of unevaluated studies and the prevalence of reproducible research practices. 
In contrast, comparatively less resource intensive indirect assessments that catalog reproducible research practices can be more easily scaled.
By evaluating many studies, indirect assessments provide evidence of how common reproducible research practices are and, by extension, how reproducible work in a field of study is likely to be.
The trade off being that indirect assessments do not offer as detailed a look into why a particular result in a specific instance could, or could not, be obtained.

Despite their growing number, the available body of direct and indirect reproducibility assessments currently provides insufficient evidence to conclusively evaluate the reproducibility of research generally, or disciplinary research specifically \citep{NASEM2019}.
This knowledge gap exists because neither direct nor indirect reproducibility assessments have been designed so the results of the sample of studies evaluated can be extended to the population of research as a whole. 
Reproduction attempts remain relatively rare, and those that do exist \citep[see][]{chang2015economics, moraila2014measuring} have selectively targeted publications in high-impact journals and conference papers series in a limited number of disciplines that are not representative of many forms of research.    
Indirect reproducibility assessments that catalog the availability of research components similarly focus on conveniently accessible archives, such as conference paper series \citep{gundersen2018state, ostermann2017}, or target specific journals and disciplinary repositories \citep{stodden2018empirical, stodden2018enabling, byrne_2017}.
These assessments have also tended to focus on evaluating the computational components of research such as data and code availability, as this information is easier to identify and assess at scale when compared to components like field procedures or qualitative analysis that may be included only in text. 
Surveys of researcher practices and attitudes on reproducibility face similar challenges to their generalizability. 
Rather than carefully defining researcher populations of interest, prior surveys have drawn data non-randomly from self-selecting populations that are convenient to survey (e.g., members of professional association, conference attendees), but are not necessarily representative of researchers active in a particular field. 
These surveys have also commonly failed to systematically report the methodological details (e.g., response rate) needed to assess and address potential bias in survey response. 

While it may not be possible to determine the extent of irreproducibility across or within disciplines, it is possible to draw lessons about potential sources of irreproducibility from available studies.
The evidence available from direct and indirect reproducibility assessments suggests inadequate record keeping, opaque reporting, unavailable research components, and a lack of incentives are important factors contributing to the irreproducibility of research \citep{NASEM2019}. 
Prominent attempts to reproduce research in economics \citep{chang2015economics}, computer science \citep{moraila2014measuring}, and psychology \citep{open2015estimating} have all been able to reproduce only a small portion of studies because original authors did not share data or adequate information about their procedures. 
Systematic reviews of publications in prominent outlets \citep{byrne_2017,stodden2018empirical} suggest these issues are likely widespread, as the majority of articles reviewed did not share data adequate for reproducibility.

Surveys of researchers about reproducibility issues and related research practices paint a similar picture. 
Recent surveys suggest that the majority of scientists have either not tried, or tried and failed to reproduce another scientist's work \citep{baker20161, boulbes2018survey}. 
Moreover, a sizable portion of the researchers responding to these surveys admit to engaging in questionable research practices linked to the publication of false positive results and low reproducibility rates \citep{fanelli2009many, fraser2018questionable}.
For example, in a survey of 807 ecologists and evolutionary biologists, 64 percent admitted to failing to report non-significant results and 51 percent admitted to presenting unexpected findings as if they been hypothesized from the start of the research \citep{fraser2018questionable}.  
These findings also match researcher perceptions, which link irreproducibility to causes ranging from natural variation to outright fraud \citep{ranstam2000fraud, anderson2007normative, baker20161}. 


\subsection*{Assessing the Reproducibility of Geographic Research}
To date, only a small number of studies have attempted to reproduce geographic research.
The reproductions that are available within the discipline suggest that many studies cannot be fully reproduced \citep{Kedron2021ssrn, nust2018, ostermann2021}, or are simply missing the components needed to attempt a reproduction \citep{Kedron_VijayanRP, konkol2019, ostermann2021}.




For example, \citet{ostermann2021}'s attempt to reproduce the computational analyses of 75 papers published in the GIScience conference series found that most results could not be reproduced, or could only be reproduced with significant effort. 
A related effort to reproduce 31 papers published in the conference series of the Association of Geographic Information Laboratories in Europe \citep{Nust_AGILE_2020, Nust2021AGILE, Nust_AGILE_2022} similarly identified the inaccessibility of data, code, and information about the computational environment as key barriers to recreating results. 
\citet{konkol2019}'s attempt to reproduce of 41 geocomputational studies likewise found that 39 of the papers had coding issues that hindered reproduction and that 47 percent of the figures created using the original data and code deviated from the those published in some significant way (e.g., graphs with different curves). 

Moving past the question of whether the computational results of a study can be recreated using the same data and code, a handful of geographic researchers are using the reproduction process as a way of examining not only the execution of a prior study, but its conceptualization and inferences \citep{Kedron_MollaloRP, Kedron_SaffaryRP, Kedron_VijayanRP}. 
While this form of the direct approach to reproduction opens a new door to the critical evaluation of geographic research, it primarily provides evidence about a particular research study and the phenomenon that study investigates. 
Similarly, while computationally focused reproductions have produced useful guidelines \citep{hofer2019reproducible, wilson2021} and moved forward the development of the computational and institutional infrastructure \citep{Kedron_Holler_Bardin_Hilgendorf_2022, nust2019, nust2021,}\textbf{ADD SHAOWEN} needed to foster reproducibility in the field, they have not yet assessed the use of reproducible research practices across the disciplines diverse research traditions, or identified the factors that have barred geographers from adopting practices that foster reproducibility or from conducting more reproduction studies. 


To date only a handful studies have tried to indirectly measure the reproducibility of geographic research.
In a survey of participants from the 2016 European Geosciences Union General Assembly, \citet{konkol2019} assessed the frequency with which researchers published their work in ways that enabled computational reproducibility. 
Those authors found that only 33 percent of respondents included links to the data used in their analyses and only 12 percent provided their analytical code. 
The authors also found that only seven percent of respondents ever attempted to reproduce the work of other researchers.
For example, \citet{ostermann2017} use a literature review of volunteered geographic information research publications to assess computational reproducibility based on availability of original data, metadata, source code, or pseudocode.
Remote Sensing survey.

\subsubsection*{Research Summary to Transition}
The evidence available from direct and indirect assessments of reproducibility suggests that studies are often not reproducible because researchers do provide the information or materials others need to recreate their work. 



However, may of our indirect assessment are on shaky methodological grounds and we have not studied this in geography. 

\begin{description}
\setlength{\itemindent}{0ex}
    \item[\textbf{(Q1)}] How do geographic researchers define reproduciblity?
    \item[\textbf{(Q2)}] How familiar are geographic researchers with different reproducible research practices?
    \item[\textbf{(Q3)}] How often do geographic researchers use different reproducibile research practices when conducting their own studies? 
    \item[\textbf{(Q4)}] What factors hinder the reproduction of geographic research?
\end{description}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Data and Methods}
Complete documentation of the procedures and materials used in this study are available through the Survey of Reproducibility in Geographic Research Repository (\citet{Kedron_Holler_Bardin_Hilgendorf_2022} - \url{https://osf.io/5yeq8/}) hosted by the Open Science Framework (OSF).
That repository connects to a Github repository which hosts the anonymized dataset and code used to create all results and supplemental materials and along with a complete history of their development. 
Before the start of data collection, we registered a preanalysis plan for the survey with OSF Registries (\citet{Kedron_Survey_PAP} - \url{https://osf.io/6zjcp}). 
We later amended that plan to reflect a change in stopping rule we used to end the survey and to include additional statistical analyses we conducted after viewing the survey response. 

The survey was conducted under the approval and supervision of the Arizona State Institutional Review Board - \textit{STUDY00014232}.
All approved documentation, study protocols, and consent materials are available through the above repository.

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Survey Design}
Our target population of interest is researchers who have recently published in the field of geography. 
We followed a 4-step procedure to create a sampling frame for our survey that captures this diverse population of researchers and the approaches they use when studying geography. 

First, beginning at the publication level, we identified journals indexed as either geography or physical geography by the \href{https://access.clarivate.com/}{Web of Science's Journal Citation Reports} that also had a 5-year impact factor greater than 1.5.
From those journals, we created a database of all articles published between 2017 and 2021.  

Second, we used Arizona State University's institutional subscription to the \href{https://www.scopus.com/home.uri}{Scopus Database} to extract journal information (e.g., subject area, ranking), article information (e.g., abstract, citation counts), and author information (e.g., corresponding status, email) for each publication. 
Because our intention is to capture individuals actively publishing new geographic research, we retained publications indexed by Scopus as \textit{document type = "Article"} and removed all other publication types (e.g., editorials, book reviews) from our article database. 
We also removed articles with missing authorship information. 

Third, we next moved to the author level to create a condensed list of corresponding authors. 
We chose to focus on corresponding authors for two reasons. 
(1) Corresponding authorship is one indicator of the level of involvement an individual had in a given work. 
While imperfect, it was the best available indicator in the Scopus database as across journals there is no commonly adopted policy for declarations of author work (e.g., CRediT Statements).
(2) Scopus maintains email contact information for all corresponding authors, which gave us a means of contacting researchers in our sampling frame.
Scopus also maintains a unique identifier for each author (author-id) across time, which allowed us to identify authors across publications. 

Fourth, we associated each corresponding author with their contact email address and deduplicated to create a single record for each corresponding author. 
To maximize our chance or response, we retained the latest available contact information for each author by sorting the initial list by author-id and publication year (descending) and keeping the latest available entry for each author-id. 
For authors who had two or more distinct emails in the latest year of publication, we deduplicated by giving a ranked preference to .edu, .gov, and then .org extensions.

Applying these criteria yielded a sampling frame of 29,828 authors. 
On average, these authors published 2.7 articles in geography journals meeting our criteria between 2017 and 2021. 
Roughly one-third (33 percent) were most recently a corresponding author for an article published in a general geography journal. 
A similar proportion (32 percent) were most recently a corresponding author for an article published in an earth sciences journal, and smaller proportions in the social sciences and cultural geography (20 percent and 16 percent, respectively).

The survey instrument used in this study is available through the Survey of Reproducibility in Geographic Research Repository.
The survey consists of 23 questions that assess (i) perceptions of the reproducibility of geographic research, (ii) familiarity and use of reproducible research practices, and (iii) beliefs about barriers to reprodicibility. 
Survey questions were developed following a review of prior reproducibility surveys \citep[e.g.,][]{fanelli2009many,baker20161, konkol2019} and our own reading of recurring issues in the reproducibility literature. 
Because this survey had not been previously fielded, we pilot tested the instrument with a subset of \textit{n}=19 graduate students and geography faculty with differing levels of experience, topical focus, and methodological background. 
After pilot testing we removed these individuals from our sampling frame to ensure they would not be included in our final sample.

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Data Collection}
We used a digital form of the Tailored Design Method \citep{dillman2014internet} to survey geographic researchers between May 17 and June 10, 2022.
A simple random sample of 2,000 researchers was drawn without replacement from our sampling frame, and those researchers were invited via email to participate in the online survey. 
Researchers received their initial invitation on May 17, 2022. 
Two reminder emails were sent to researchers that had not yet completed the survey on May 26 and May 31, 2022.

Participation in the survey was entirely voluntary. 
Each researcher that opted to participate in the survey was provided with IRB approved consent documentation and linked to the internet survey instrument. 
Participants were also given the option to enter a random prize draw. 
The total possible compensation a participant was eligible to receive was 90 US dollars, awarded as a prepaid credit card.
Three prize winners were selected at random from those electing to enter the prize draw at the end of the data collection period.

The online survey was administered through \href{https://www.qualtrics.com/}{Qualtrics}. 
Each question on the survey was presented as a unique page. 
Adaptive questioning, conditionally displaying items based on responses to other items, was used to reduce the number of complexity of questions.
Participating researchers had the option to exit and re-enter the survey and were also able to review and/or change their answers using a back button as they progressed through the survey.

At the end of the data collection period, responses were checked for completeness and coded using the reporting standards of the American Association For Public Opinion Research \citep{aaporstandards}.
Responses were downloaded from Qualtrics, anonymized, and stored in a password-protected databases.
We used a unique key to link this data with participant information (e.g., citation history) stored in a separate database during our statistical analyses. 
To preserve participant privacy that connection was severed at the end of our analysis.
We have only shared the anonymized response file through our public repository. 
As a result some of the result presented below cannot be directly replicated. 
We have indicated throughout our results section which analysis can and cannot be replicated using the data files and code share through our public repository. 

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Survey Response Demographics}
A total of \textit{n}=218 of the authors we contacted completed the online survey with information sufficient for analysis. 
The contact rate for the survey was 13.9 percent, the response rate was 10.8 percent, yielding a cooperation rate of 77.6 percent. 
The refusal rate was 3.1 percent.\endnote{All outcome rates are reported using \citet{aaporstandards} standards. The outcome rates used were - response rate 2, cooperation rate 2, refusal rate 1, and contact rate 1.}
Respondents were predominantly male (65.1\%) and between the ages of 35 and 55 (62.4\%). 
The majority of respondents were also academics, but were well balanced across career levels as no one category made up more that 30 percent of the sample.
Respondents were similarly well balanced across the disciplinary subfields - Human Geography (30.7\%), Physical Geography (29.8\%), Nature and Society (10.1\%), Geographic Methods and GISci (28.0\%); and methodological approaches - quantitative (42.2\%), qualitative (18.3\%), and mixed-methods (39.0\%) research.

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Statistical Methods}
We conduct two statistical analyses of the survey response. 
First, we used descriptive statistical summaries of the participant response to address our four questions about how geographic researchers define reproducibility, their familiarity and experience with reproducible research practice, and the barriers to reproducibility they perceive in the discipline.
For each question, we created a single variable that aggregates a participant's response across a subset of survey questions related to that issue.

We measured each participant's definition of reproducibility relative to the definition provided by the \citet{NASEM2019}. 
Before being provided a standardized definition, each participant was asked to provide their own definition of reproducibility. 
We coded these text responses along four dimensions of data, procedure, result, and context similarity.  
To assess researcher familiarity with reproducible practices, we created a single variable, \textit{Familiarity}, that counts the number of practices a researcher was 'somewhat' or 'very' familiar with.
To assess researcher experience with reproducible practices, we created a single variable, \textit{Experience}, that counts the number of practices a researcher use 'most of the time' or 'always' when conducting their own research. 
Our \textit{Familiarity} and \textit{Experience} variables both range from 0-5. 
We similarly created a single measure, \textit{Barriers} that counts the number of factors a researcher believe 'frequently' or 'occasionally' contributed to a lack of reproducibility. 
This \textit{Barriers} variable ranges from 0-12.



We also examine group-wise differences in each aggregated measure by disciplinary subfield and methodological approach to identify variation across the discipline's many traditions.
This analysis is based on our full sample (\textit{n}=218), but excludes 29 participants who did not provide complete responses to the relevant survey questions.

Second, we conduct an analysis of the experience of geographic researchers who have attempted reproductions of prior work. 
A total of \textit{n}=102 of the authors we contacted reported attempting to reproduce a prior study. 
We examine what motivates geographic researchers to attempt to reproduce another study and how successful researcher attempting reproductions were at recreating existing results. 
We also investigate why researchers were unable to reproduce results.

The full methodological details of our statistical analyses, and a complete history of our response coding a variable construction is included in the Github repository linked to OSF repository that accompanies this paper.
All of the results presented in this paper can be reproduced using the materials in that repository.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Results}

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Aggregate Measures Along 4-Dimensions}
This subsection will pivot on Table 9 that has rows representing aggregate measures of 1) definition, 2) Familiarity 3) Experience, 4) Barriers and two groups of columns representing (Quant, Qual, Mixed) and (Human, Physical, GIS/Methods, NS).

\begin{table}[h]
    \centering
    \begin{threeparttable}
    \caption{Descriptive summary of survey results}
    \begin{tabular}{l c c c c c c c c c c}
         \hline
                    & \multicolumn{4}{1}{Subfield}  & & \multicolumn{3}{1}{Approach} & & \\
         Measure    & HU & PH & NS & GIS            & & QNT & QUL & MIX              & & Overall \\
         \hline
         Definition  &  & & & & & & & & & \\
         Familiarity &  \begin{tabular}[c]{@{}c@{}}2.35\\ (1.36)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}3.71\\ (0.98)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}2.28\\ (1.76)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}3.97\\ (0.86)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}3.75\\ (0.98)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}1.75\\ (1.30)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}3.56\\ (1.22)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}3.26\\ (1.36)\end{tabular}\\
         
         Experience  &  \begin{tabular}[c]{@{}c@{}}0.60\\ (0.80)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}2.06\\ (1.18)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}1.18\\ (0.96)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}1.85\\ (1.38)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}1.63\\ (1.32)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}0.48\\ (0.68)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}1.69\\ (1.24)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}1.44\\ (1.28)\end{tabular}\\
         
         Barriers    &  \begin{tabular}[c]{@{}c@{}}7.32\\ (3.57)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}8.31\\ (3.00)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}7.44\\ (3.20)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}9.31\\ (2.79)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}8.78\\ (2.80)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}5.97\\ (3.83)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}8.58\\ (2.96)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}8.20\\ (3.24)\end{tabular}\\
                     & & & & & & & & & &\\
         N           & 67 & 65 & 22 & 61 & & 92 & 40 & 85 & & 218 \\
         Missing     & 14 & 4  & 4  & 6  & & 6 & 9 & 14 & & 29 \\
        \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item \textit{HU} Human Geography, \textit{PH} Physical Geography, \textit{NS} Nature and Society, \textit{GIS} GIScience and Methods; \textit{QNT} Quantitative, \textit{QUL} Qualitative, \textit{MIX} Mixed Methods.
    \end{tablenotes}
    \label{tab:overview}
    \end{threeparttable}
\end{table}

\begin{itemize}
    \item Overall story is that collectively geographers are aware of reproducible research practices, but don't have a matching amount of direct experience using those practices. There are also many perceived barriers to conducting reproducible research.
    \item High - Low gradient across all aggregate measures from Quant-MM-Qual.
    \item High - Low familiarity gradient Method-Physs-NS-H
    \item Likely need to comment on survey including many reproducible research practices that center on, or are easier to accomplish with, when using quantitative data and methods. This reality may automatically deflate the aggregate measures of certain sub-group. We can use the response to Q8f as a filter to explore this issue. \textcolor{red}{For conclusions - Points to the emphasis on quantitative, particularly computational, focus in the reproducibility literature. Suggests a meed to examine what reproducibility means in the context of geography's qual, NS, and human traditions.} 
\end{itemize}

%%%%%%%%%%%
\subsubsection*{Definition of Reproducibility}
Coded definitions are relative to the NASEM definition. The NASEM state SAME data, procedures, and results and implies SAME context because data is not recollected. 
\begin{itemize}
    \item Self-reported definition (Q6) analysis: Qualitative analysis that will present a typology of definitions by combination of the Q,Q,M and M,P,NS,H splits that match on the 8 category coding of the provided definitions. 
    \item Interpretations (Q8) and Importance (Q17) of reproductions: Develop a split that distinguishes between Qs that focus on epistemology Q8a+Q8b+Q8f+17a, as these provide the most direct information about how a respondent processes the evidence provided by a reproduction; and Qs that focus on reproducibility as a quality of a study Q8e+Q8c+17c+17e+17f. We will explore how these question groups crosstab with the typologies we draw from Q6. Initial assumption is that many will support reproduction as a study characteristic or label, and less will link it with epistomological function. Examine by subfields and methods, which we expect to align with typologies from Q6. 
    \item Importance of reproducibility (Q17) - Reproducibility was/wasn't seen as important for other open science goals - error reduction (17b), effort duplication (17d), scientific communication (17g, 17h), training (17i, 8d).
    \item Reproducibility Estimate (Q13) - \textcolor{red}{What is the state of the summaries on this dimension?}
\end{itemize}

%%%%%%%%%%%
\subsubsection*{Reproducible Research Practices and Barriers to Reproducibility}
This will be a discussion of how subfields differ on the specific questions used to create the aggregate variables presented above. 
The purpose is to enrich the overall picture by identifying differences within the aggregate measures.
\begin{itemize}
    \item Use Q9 to contextualize the frequency measure.
    \item Use details from Q14 to enrich the discussion of barriers
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Actual Reproductions}
A total of \textit{n}=102 of the authors we contacted reported attempting to reproduce a prior study.

Start with the outline of what the population is here because not all respondents answered these questions. Start with descriptives that show this sub-sample of respondents. Summary discussion of Q10-Q12.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Discussion}

\begin{itemize}
    \item The reproducibility literature emphasizes quantitative, particularly computational, forms of research. This emphasis necessarily carried over into our survey instrument, which may automatically lower response scores for researchers not working in these areas (e.g., qualitative, human geography). These scores do reflect their present level of understanding, but the practices may not be relevant to their methods or epistomologies. This fact suggests a meed to examine what reproducibility means in the context of geography's qual, NS, and human traditions.
    \item A standing question is how should geographic research approaches be designed to efficiently generate reliable knowledge.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Conclusion}


\theendnotes


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgement(s)}
We thank Tyler Hoffman for providing technical assistance in the development and execution of a set of trial queries using the Scopus API.

\section*{Funding}
This material is based on work supported by the National Science Foundation under Grant No. \textbf{BCS-2049837}.

\section*{Notes on contributor(s)}
\textbf{Kedron:} Conceptualization, Methodology, Writing - Original Draft, Writing - Review and Editing, Supervision, Project Administration, Funding Acquisition. \textbf{Holler:} Conceptualization, Methodology, Data Curation, Writing - Review and Editing, Funding Acquisition. \textbf{Bardin:} Conceptualization, Methodology, Writing - Original Draft, Writing - Review and Editing, Data Curation, Software.



\newpage
\bibliography{references}

\newpage
\noindent PETER KEDRON is an Associate Professor in the School of Geographical Science and Urban Planning and core faculty member in the Spatial Analysis Research Center (SPARC) at Arizona State University, Tempe, AZ, 85283, US. Email: Peter.Kedron@asu.edu. His research interests include spatial analysis, geographic information science, economic geography, and the accumulation of knowledge about geographic phenomena. \\  
  
\noindent JOSEPH HOLLER is an Assistant Professor of Geography at Middlebury College, Middlebury, VT, 05753, US. Email: \\
  
\noindent SARAH BARDIN is a PhD candidate ...

\end{document}
