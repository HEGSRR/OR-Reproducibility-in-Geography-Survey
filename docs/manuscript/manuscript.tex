\documentclass[]{interact}
\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.
\usepackage{subfigure}% Support for small, `sub' figures and tables
%\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required

\usepackage{natbib}
\bibliographystyle{chicago}
\setcitestyle{authoryear,open={(},close={)}}
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}% Bibliography support using natbib.sty

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=blue,
}

\usepackage{titlesec}
\titleformat*{\section}{\Large\bfseries}
\titleformat*{\subsection}{\large\bfseries}

\usepackage{endnotes}
\let\footnote=\endnote
\usepackage{etoolbox}
\patchcmd{\enoteformat}{1.8em}{0pt}{}{}

\theoremstyle{plain}% Theorem-like structures provided by amsthm.sty
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}

\usepackage{tabularx}
\usepackage{booktabs,caption}
\usepackage{threeparttable}

\usepackage{lscape}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\articletype{DRAFT MANUSCRIPT}

\title{Reproducible Research Practices and Barriers to Reproducible Research in Geography: Insights from a Survey}

\author{
\name{Peter Kedron\textsuperscript{a,b}\thanks{CONTACT Peter Kedron. Email: Peter.Kedron@asu.edu}, Joseph Holler\textsuperscript{c}, and Sarah Bardin\textsuperscript{a,b}}
\affil{\textsuperscript{a}School of Geographical Sciences and Urban Planning, Arizona State University, Tempe, Arizona, USA; \textsuperscript{b}Spatial Analysis Research Center (SPARC), Arizona State University, Tempe, Arizona, USA; \textsuperscript{c}Department of Geography, Middlebury College, Middlebury, Vermont, USA}
}

\maketitle

\begin{abstract}
While the number of reproduction and replication studies undertaken in the social and behavioral sciences continues to rise, such studies have not yet become commonplace in geography. 
Existing attempts to reproduce geographic research suggest that many studies cannot be fully reproduced, or are simply missing components needed to attempt a reproduction. 
Despite this suggestive evidence, we have not yet systematically assessed geographers' perceptions of reproducibility, the use of reproducible research practices across the discipline's diverse research traditions, or identified the factors that have kept geographers from conducting more reproduction studies.
This study addresses each of these questions by surveying active geographic researchers selected using probability sampling techniques from a rigorously constructed sampling frame.
We find ...
CONClUSION...

\end{abstract}

\begin{keywords}
Reproducible Research, Epistemology, Geographic Research Methods
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{Introduction}
Reproducibility is important for geography for the same reason it is important in other fields - one-off studies 
Reproducibility as a characteristic of a study that lends it credibility v. Reproducibility as a error correcting function of science. 


While geographers continue to debate the role of reproduction studies in the discipline \citep{kedron2022replication}, examine the reproducibility of individual studies \citep{Nust_AGILE_2022, kedron2021GA}, and build the infrastructure needed to support reproducible research \citep{wilson2021}, no one has conducted a systematic survey of the use of reproducible research practices in the discipline. 

Within geography, reproduction efforts have identified the inaccessibility of data, code, and information about the computational environment as key barriers to recreating results. 
Irreproducibility has also been linked to conceptual and methodological transparency as researchers make decisions about the handling of spatial forms of uncertainty \citep{kedron2021GA}. 
While these efforts have produced useful reproducibility guidance for geographic researchers \citep{hofer2019reproducible, wilson2021} they have not been systematic surveys of the discipline as a whole. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Assessing the Reproducibility of Research}

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{The Evidence Currently Available from Reproducibility Assessments}
The reproducibility of research can be assessed directly by attempting to recreate the results of selected studies, or indirectly by reviewing the availability of the research components (e.g., data and code) or surveying those working in that field about their research practices. 
Direct assessments of reproducibility provide insight into how a particular study was conducted and why a consistent result could, or could not, be obtained.  
However, such assessments are resource and time intensive, which makes it difficult to gather information from numerous studies. 
Constrained to a limited number of studies, direct assessments offer only suggestive evidence about the reproducibility of unevaluated studies and the prevalence of reproducible research practices. 
In contrast, comparatively less resource intensive indirect assessments that catalog reproducible research practices across many studies offer less insight into individual studies, but provide evidence of how common reproducible research practices are and, by extension, how reproducible work in a field of study is likely to be.

Across many disciplines, direct reproduction attempts remain relatively rare \citep[see][]{chang2015economics, moraila2014measuring, iqbal2016reproducible,sumner2020reproducibility}.
However, prominent attempts to reproduce research in economics \citep{chang2015economics}, computer science \citep{moraila2014measuring}, and psychology \citep{open2015estimating} have all been able to reproduce only a small portion of studies because original authors did not share data or adequate information about their procedures.

Within geography, the small number of available reproduction attempts similarly suggest that many studies cannot be fully reproduced, or are simply missing the components needed to attempt a reproduction \citep{nust2018, Kedron2021ssrn, konkol2019, ostermann2021, Nust_AGILE_2022}.
For example, \citet{ostermann2021}'s attempt to reproduce the computational analyses of 75 papers published in the GIScience conference series found that most results could not be reproduced, or could only be reproduced with significant effort. 
\citet{konkol2019}'s attempt to reproduce 41 geocomputational studies likewise found that 39 of the papers had coding issues that hindered reproduction and that 47 percent of the figures created using the original data and code deviated from the those published in some significant way (e.g., graphs with different curves). 
A series of reproduction attempts by \citet{Kedron_MollaloRP, Kedron_SaffaryRP, Kedron_VijayanRP} and \citet{paez2022reproducibility} show that the issues hindering the reproduction of results extend beyond computation and into the conceptualization and design of geographic research.
Nonetheless, like reproduction attempts in other disciplines these studies provide only suggestive evidence of underlying problems, not a comprehensive assessment of the prevalence or sources of these issues.

Indirect reproducibility assessments offer a wider view of what and how common these underlying issues may be. 
The available evidence for study reviews suggests inadequate record keeping, opaque reporting, unavailable research components, and a lack of incentives are likely important factors contributing to the irreproducibility of research \citep{NASEM2019}. 
Systematic reviews of publications in prominent outlets \citep{byrne_2017,stodden2018empirical} suggest these issues are likely widespread. 
Reproducibility surveys paint a similar picture. 
Recent surveys suggest that the majority of scientists have either not tried, or tried and failed to reproduce another scientist's work \citep{baker20161, boulbes2018survey}. 
Moreover, a sizable portion of the researchers responding to these surveys admit to engaging in questionable research practices linked to the publication of false positive results and low reproducibility rates \citep{fanelli2009many, fraser2018questionable}.
These findings match researcher perceptions, which link irreproducibility to causes ranging from natural variation to outright fraud \citep{ranstam2000fraud, anderson2007normative, baker20161}. 

To date only a handful studies have attempted to indirectly measure the reproducibility of geographic research.
In a survey of remote sensing editors and working group officers of the International Society for Photogrammetry and Remote Sensing, \citet{balz2020reproducibility} asked respondents whether they believe a reproducibility crises existed in their sub-field, but not what the causes of such a crisis might be.
In a richer treatment of the issue, \citet{konkol2019} survied of participants from the 2016 European Geosciences Union General Assembly to assess the frequency with which researchers published their work in ways that enabled computational reproducibility. 
Those authors found that only 33 percent of respondents included links to the data used in their analyses and only 12 percent provided their analytical code. 
The authors also found that only seven percent of respondents ever attempted to reproduce the work of other researchers.
Similarly, in a literature review of volunteered geographic information research, \citet{ostermann2017} found the computational reproducibility of work in the sub-field limited by the availability of original data, metadata, source code, or pseudocode.






%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Limitations of the Currently Available Evidence}
While it may not be possible to determine the extent of irreproducibility across or within disciplines from available studies, it is possible to draw lessons about potential sources of irreproducibility.





%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Current State of Knowledge}

Most indirect assessments sample studies for review from conveniently accessible archives, such as conference paper series \citep{gundersen2018state, ostermann2017}, or target specific journals and disciplinary repositories \citep{stodden2018empirical, stodden2018enabling, byrne_2017}.
These assessments have also tended to focus on evaluating the computational components of research such as data and code availability, as this information is easier to identify and assess at scale when compared to components like field procedures or qualitative analysis that may be included only in text. 
Surveys of researcher practices and attitudes on reproducibility face similar challenges. 
Rather than carefully defining researcher populations of interest, existing surveys have drawn data non-randomly from self-selecting populations that are convenient to survey (e.g., members of professional association, conference attendees), but are not necessarily representative of researchers active in a particular field. 
These surveys have also commonly failed to systematically report the methodological details (e.g., response rate) needed to assess and address potential bias in survey response. 

Across many disciplines, direct reproduction attempts remain relatively rare, and those that do exist \citep[see][]{chang2015economics, moraila2014measuring, iqbal2016reproducible,sumner2020reproducibility} have selectively targeted publications in high-impact journals and conference papers series in a limited number of disciplines that are not representative of many forms of research.

Despite their growing number, the available body of both direct and indirect reproducibility assessments currently provides insufficient evidence to conclusively evaluate the reproducibility of research generally, or disciplinary research specifically \citep{NASEM2019}.
This knowledge gap exists because neither direct nor indirect reproducibility assessments have been designed so the results of the sample of studies evaluated can be extended to the population of research as a whole.

Overall, we have yet to assess the use of reproducible research practices across the discipline's diverse research traditions, or identify the factors that have barred geographers from adopting practices that foster reproducibility or from conducting more reproduction studies.
Our knowledge and systems remain incomplete because 

the evidence available from direct and indirect assessments of reproducibility suggests that studies are often not reproducible because researchers do provide the information or materials others need to recreate their work. 

Which leaves a set of basic questions about the reproducibility of geographic research unanswered.  


\begin{description}
\setlength{\itemindent}{0ex}
    \item[\textbf{(Q1)}] How do geographic researchers define reproduciblity?
    \item[\textbf{(Q2)}] How familiar are geographic researchers with different reproducible research practices?
    \item[\textbf{(Q3)}] How often do geographic researchers use different reproducibile research practices when conducting their own studies? 
    \item[\textbf{(Q4)}] What factors hinder the reproduction of geographic research?
\end{description}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Data and Methods}
Complete documentation of the procedures, survey instrument, and other materials used in this study are available through the Survey of Reproducibility in Geographic Research Repository (\citet{Kedron_Holler_Bardin_Hilgendorf_2022} - \url{https://osf.io/5yeq8/}) hosted by the Open Science Framework (OSF).
That repository connects to a Github repository which hosts the anonymized dataset and code used to create all results and supplemental materials along with a complete history of their development. 
All of the results presented in this paper can be independently reproduced using the materials in that repository.
Before the start of data collection, we registered a preanalysis plan for the survey with OSF Registries (\citet{Kedron_Survey_PAP} - \url{https://osf.io/6zjcp}). 
The survey was conducted under the approval and supervision of the Arizona State Institutional Review Board - \textit{STUDY00014232}.

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Sampling Frame}
Our target population of interest is researchers who have recently published in the field of geography. 
We followed a 4-step procedure to create a sampling frame for our survey that captures this diverse population of researchers and the approaches they use when studying geography. 

First, beginning at the publication level, we identified journals indexed as either geography or physical geography by the \href{https://access.clarivate.com/}{Web of Science's Journal Citation Reports} that also had a 5-year impact factor greater than 1.5.
From those journals, we created a database of all articles published between 2017 and 2021.  

Second, we used Arizona State University's institutional subscription to the \href{https://www.scopus.com/home.uri}{Scopus Database} to extract journal information (e.g., subject area, ranking), article information (e.g., abstract, citation counts), and author information (e.g., corresponding status, email) for each publication. 
Because our intention was to capture individuals actively publishing new geographic research, we retained publications indexed by Scopus as \textit{document type = "Article"} and removed all other publication types (e.g., editorials, book reviews) from our article database. 
We also removed articles with missing authorship information. 

Third, we created a list of researchers and their published articles, focusing on corresponding authors for two reasons.
(1) Corresponding authorship is one indicator of the level of involvement an individual had in a given work. 
While imperfect, it was the best available indicator in the Scopus database as across journals there is no commonly adopted policy for declarations of author work (e.g., CRediT Statements).
(2) Scopus maintains email contact information for all corresponding authors, which gave us a means of contacting researchers in our sampling frame.
Scopus also maintains a unique identifier for each author (author-id) across time, which allowed us to identify authors across publications. 

Fourth, we constructed a sampling frame of unique researchers and their most recent email contact information. 
We determined uniqueness by grouping researchers by their author-id, and we selected for the most recent contact information by selecting records associated with the most recent year of publication. 
For 383 researchers who had two or more distinct emails in the latest year of publication, we removed non-institutional personal email addresses and then selected one of the remaining institutional email address.

Applying these criteria yielded a sampling frame of 29,828 researchers. 
On average, these authors published 2.7 articles in geography journals meeting our criteria between 2017 and 2021. 
Roughly one-third (33.0\%) were most recently a corresponding author for an article published in a general geography journal. 
A similar proportion (32.0\%) were most recently a corresponding author for an article published in an earth sciences journal, and smaller proportions in the social sciences and cultural geography (20.0\% and 16.0\%, respectively).

\subsection*{Survey Instrument}
The survey consisted of 23 questions that assess (i) perceptions of the reproducibility of geographic research, (ii) familiarity and use of reproducible research practices, and (iii) beliefs about barriers to reproducibility, and (iv) experience conducting independent reproductions. 
Survey questions were developed following a review of prior reproducibility surveys \citep[e.g.,][]{fanelli2009many,baker20161, konkol2019} and our own reading of recurring issues in the reproducibility literature. 
All researchers were presented questions about their definition of reproducibility, familiarity with and use of reproducible practices, and perceptions of barriers to reproducibility. 
A subset of researchers that reported attempting to reproduce a study in the past two years were given an additional set of questions about their experience conducting that work.

We pilot tested the survey instrument with a subset of \textit{n}=19 graduate students and geography faculty with differing levels of experience, topical focus, and methodological background. 
After pilot testing we removed these individuals from our sampling frame to ensure they would not be included in our final sample.

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Data Collection}
We used a digital form of the Tailored Design Method \citep{dillman2014internet} to survey geographic researchers between May 17 and June 10, 2022.
A simple random sample of 2,000 researchers was drawn without replacement from our sampling frame, and those researchers were invited via email to participate in the online survey. 
Researchers received their initial invitation on May 17, 2022. 
Two reminder emails were sent to researchers that had not yet completed the survey on May 26 and May 31, 2022.

Participation in the survey was entirely voluntary. 
Each researcher that opted to participate in the survey was provided with IRB approved consent documentation and linked to the internet survey instrument. 
Participants were also given the option to enter a random prize draw. 
The total possible compensation a participant was eligible to receive was 90 US dollars, awarded as a prepaid credit card.
Three prize winners were selected at random from those electing to enter the prize draw at the end of the data collection period.

The online survey was administered through \href{https://www.qualtrics.com/}{Qualtrics}. 
Each question on the survey was presented as a unique page. 
Adaptive questioning, conditionally displaying items based on responses to other items, was used to reduce the number of complexity of questions.
Participating researchers had the option to exit and re-enter the survey and were also able to review and/or change their answers using a back button as they progressed through the survey.

At the end of the data collection period, responses were checked for completeness and coded using the reporting standards of the American Association For Public Opinion Research \citep{aaporstandards}.
Responses were downloaded from Qualtrics, anonymized, and stored in a password-protected databases.
We used a unique key to link this data with participant information (e.g., citation history) stored in a separate database during our statistical analyses. 
To preserve participant privacy that connection was severed at the end of our analysis.
We have only shared the anonymized response file through our public repository. 
As a result some of the result presented below cannot be directly replicated. 
We have indicated throughout our results section which analysis can and cannot be replicated using the data files and code share through our public repository. 

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Analytical Approach}

%%%%%%%%%%%
\subsubsection*{Statistical Approach}
We conduct two statistical analyses of the survey response.
First, we create descriptive statistical summaries of the participant response to address how geographic researchers define reproducibility, their familiarity and experience with reproducible research practices, and the barriers to reproducibility they perceive in the discipline.
Second, we conducted an analysis of the experience of the subset of geographic researchers who reported attempting independent reproductions of prior work during the last two years.
That analysis identified what motivated reproduction attempts, how successful those attempts were, and what factors hindered success.
Given the topical and epistemological variation that exists among geographic researchers, we examined differences by disciplinary subfield and methodological approach as part of each analysis.

%%%%%%%%%%%
\subsubsection*{Variable Construction}
For our first analysis, we constructed summary measures that aggregated researcher responses to questions linked to 1) definitions of reproducibility, 2) familiarity with and 3) use of reproducible research practices, and 4) potential barriers to reproducibility. 

\textit{Definition:} Before providing a common definition of reproducibility, we asked each participant to provide their own definition of the term.
We coded these definitions following two procedures.
First, we measured the similarity of each provided definition to the definition of adopted by the \citet{NASEM2019}. 
The NASEM defines reproducible research as having four characteristics --- same data, same procedure, same results, and same conditions.
To make this comparison, each of the authors independently coded each respondent definition for the presence/absence of each of the four characteristics included in the NASEM definition.
Disagreements in the assignment of codes were resolved through discussion between the three authors.
To create an aggregate measure of agreement the final coded response for each participant was then summed across the four characteristics, creating definition similarity \textit{d} with domain [0, 4].

Second, we coded each definition to one of three motivations for ensuring the reproducibility of a study--- to facilitate the assessment of prior work, to improve transparency and facilitate further extension of work, and to improve the transparency and consistency of study data.
We derived this coding from common themes in the authors response and our own reading of the reproducibility literature.
As above, each definition was independently coded by each author before code assignments across authors were compared with disagreements resolved through discussion.

\textit{Familiarity and Experience:} We measured participant familiarity and use of five reproduciblity enhancing research practices; the adoption of--- open-source software, research notebooks, data sharing, code sharing, and research plan preregistration. 
We created a summary measure of familiarity with these practices, \textit{f}, by summing the number of reproducible practices a researcher was 'somewhat' or 'very' familiar with.
To assess experience with these same five practices, we created \textit{p}, which counts the number of practices a researcher reported using 'most of the time' or 'always' when conducting their own research. 
Both \textit{f} and \textit{p} have domains [0,5]. 

\textit{Barriers:} Finally, we asked asked respondents to identify how frequently they believe 12 factors contributed to a lack of reproducibility in the discipline. 
From those responses, we created a summary measure, \textit{b} with domain [0,12], of the number of barriers a researcher believes 'frequently' or 'occasionally' contributed to irreproducibility. 

Our second analysis examined only researchers that reported attempting a reproduction in the past two years. 
We asked each of these researchers what motivated their reproduction attempt. 
Two of the authors independently assigned these responses to groups with the following motivations - i) verification/check of published research, ii) an internal check of their own research to verify their work and increase the transparency of their work, iii) replication of a study with new data, and iv) a desire to learn from past work, or use the reproduction in teaching. 
We identified disagreements in motivation assignments, and the third author again resolved all disputes in consultation with the two assigning authors.
We selected groups i) and iv) for further analysis of reproduction attempts, as these motivations most closely matched our definition of reproduction. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Results}

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Survey Response Demographics}
A total of \textit{n}=218 of the authors we contacted completed the online survey with information sufficient for analysis. 
The contact rate for the survey was 13.9 percent and the cooperation rate was 77.6 percent, yielding an overall response rate of 10.8 percent. 
The refusal rate was 3.1 percent\endnote{All outcome rates are reported using \citet{aaporstandards} standards. 
The outcome rates used were - response rate 2, cooperation rate 2, refusal rate 1, and contact rate 1.}.
Respondents were predominantly male (65.1\%) and between the ages of 35 and 55 (62.4\%). 
The majority of respondents were also academics, but were well balanced across career levels as no one category made up more that 30 percent of the sample.
Respondents were similarly well balanced across the disciplinary subfields - human geography (30.7\%), physical geography (29.8\%), nature and society (10.1\%), geographic methods and GIScience (28.0\%); and methodological approaches - quantitative (42.2\%), qualitative (18.3\%), and mixed-methods (39.0\%) research.

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Researcher Perspectives on Reproducibility}
A primary aim of our survey was to establish how geographic researchers conceptualize reproducibility and their current level of knowledge and experience with reproducibility enhancing research practices.
Nearly all researchers reported being at least somewhat familiar with the term reproducibility (89.0\%) with half reporting being very familiar with the term (53.6\%).
A majority of researchers also believe reproducibility is compatible with epistemologies used in the discipline (58.2\%). 
However, researchers estimated that only 50.6 percent of the results published in the discipline were reproducible, albeit with a large standard deviation of 24.7 percent that suggests great uncertainty about the true value. 
Few respondents reported attempting to reproduce the work of other researchers (14.7\%) with fewer still attempting to publish those reproduction studies (6.8\%). 

Table \ref{tab:overview} summarizes how researchers define reproducibility (\textit{d}), their familiarity with (\textit{f}) and use of reproducible research practices (\textit{u}), and the factors they see as barriers (\textit{b}) to reproducibility in geography. 
We present the mean and standard deviation for the overall sample and for each subfield and methodological approach.
In aggregate, the data reveal trends in definition, familiarity, practices, and barriers of reproducibility between the sub-disciplines and methodological approaches. 
Respondents that self-identified as specializing in physical geography and geographic methods consistently report higher indicators than those working nature and society and human geography.
Similarly, respondents that identified as primarily using quantitative and mixed methods approaches consistently report higher indicators than those using qualitative methods.
The following sub-sections present detailed results for each of these topics, highlighting the principal sources of difference between sub-disciplines and methodological approaches.

\begin{center}
\textbf{Insert Table \ref{tab:overview} About Here}
\end{center}


%%%%%%%%%%%
\subsubsection*{Definitions and Importance of Reproducibility}
A total of \textit{$n_{d}$}=181 (83.0\%) of our survey respondents provided an interpretable definition of reproducibility.  
On average, geographic researchers provided definitions of reproducibility that explicitly included one or two ($\overline{d}=1.83$) of the four characteristics from the definition adopted by the NASEM.
This level of inclusion was consistent across subfields and methodological approaches.
The availability and use of the same research procedures (80.7\%) and results (74.0\%) were the characteristics of reproducibility most frequently identified by researchers. 
Less than half of respondents explicitly included use of the same data (38.1\%) or the need to work in the same context (17.7\%) in their definitions. 
This pattern was consistent across subfields and approaches, with a slightly greater emphasis on data and procedural consistency and availability amongst quantitative and methods focused researchers.
The lower inclusion of data and context in definitions can be explained by researchers understanding reproducibility in terms of the formal NASEM definition of "replicability"--- a pattern evident in at least 20.4\% of respondents' definitions. For example, one respondent defines reproducibility as, "The extent to which the research design can be replicated in different geographical contexts."

Researchers' definitions of reproducibility were primarily connected to two epistemic functions.
Just over half of respondents (52.5\%) defined reproducibility as a means of assessing prior work for errors or inconsistencies.
Nearly all other researchers (40.9\%) tied reproducibility to the need for transparency in research so others could independently expand upon past studies.
Identification of reproducibility with both of these functions was supported by responses to related questions from our entire sample (\textit{n}=218). 
A majority of researchers identified reproducibility as important for validating (75.2\%) and establishing the credibility (72.5\%) of research.  
Respondents also saw reproducing studies as important to reducing the presence of persistent errors in the discipline (77.5\%) and to increasing trust in research findings (78.45\%).
In parallel with the need for openness and transparency in science, most respondents agreed with the importance of reproducibility for research efficiency (63.3\%), communication with academics (68.8\%) and practitioners (64.7\%), and training students (75.7\%).

Despite wide recognition of reproducibility as epistemically important, respondents were cautious about drawing conclusions from a single study or reproduction attempt. 
Half of the respondents (50.9\%) agreed that when researchers don't share their data they have less trust in a study.
41.7 percent agreed that inability to reproduce a result detracts from the validity of a study, and a smaller minority agreed that such inability implies that the result is false (26.2\%).

Qualitative researchers identified reproducibility as playing a much smaller epistemic role compared to the discipline as a whole. 
A majority of the qualitative researchers responded that reproducibility is incompatible with the epistomologies used in their subfield (75.0\%).
Accordingly small percentage of qualitative researchers agreed that reproducibility is important for validating (25.0\%) or establishing the credibility (20.0\%) of research.
Qualitative geographers similarly placed less emphasis on reproducibility as a means of increasing the accessibility and expansion of research.
Few qualitative researchers had less trust in a study when researchers did not share their data (27.5\%), or saw reproducibility as important for sharing research with academics (27.5\%).



%%%%%%%%%%%
\subsubsection*{Familiarity and Use of Reproducible Research Practices}
The reproducibility of research is on the minds of geographic researchers. 
The majority of survey respondents reported thinking about the reproducibility of their own research (80.7\%), discussing reproducibility with a colleague (70.6\%), and questioning the reproducibility of published work (57.3\%) in the past two years. 
More than half of the researchers we surveyed (52.8\%) also reported considering reproducibility while peer reviewing a grant proposal or publication during the same time frame. 

Geographic researchers are on average familiar with multiple reproducible research practices ($\overline{f}=3.26$), but reported limited use of these practices in their own work ($\overline{u}=1.44$).
More than half of all researchers reported familiarity with with data sharing (86.7\%), open source software (85.3\%), field and lab notebooks (67.0\%), and code sharing (59.2\% ).
However, while familiar with these practices, a far smaller number of researchers reported using these practices regularly in their own work. 
Less than half of the researchers surveyed reported sharing their data (44.5\%), using open source software (38.1\%), using field or lab notebooks to record their work (40.0\%), or sharing their code (18.8\%) most or all of the time. 
Only a small subset of researchers reported familiarity with the pre-registration of research designs and protocols (27.5\%), or regular use of this practice (2.7\%).

Researcher familiarity and use of reproducible research practices also varied by disciplinary subfield and methodological approach. 
Researchers that identified as physical geographers or methodologists and GIScientists reported being familiar with one to two more reproducible researcher practices than human  geographers and those focused on nature and society.
Researcher practices similarly diverged by subfield, but no subset of researchers reported using on average more than two of these practices regularly in their work.  
When grouped by methodological approach, quantitative and mixed-methods researchers on average reported familiarity with and use of two more reproducible research practices when compared to qualitative researchers.

Differences in researcher familiarity and use of specific reproducible research practices across subfields and approaches was greatest for practices more typical of quantitative workflows. 
For example, 96.7 percent of quantitative and 89.5 percent of mixed-methods researchers reported familiarity with open source software, while 52.5 percent of qualitative researchers with the same resources.
These differences were greater for code sharing practices. 
Just 12.5 percent of qualitative researchers reported familiarity with code sharing, while 81.5 percent of quantitative and 55.7 percent of mixed-methods researchers reported familiarity with the same practice.
However, even among quantitiative and mixed-methods researchers familiarity with reproducible research practices did not translate into regular use of those practices.
Just over half of quantitative (51.2\%) and mixed-methods (51.8\%) researchers reported sharing their data most of the time.
Less than a third of these same researchers reported sharing their code most of the time. 
These rates were lower for qualitative geographers, of which only a small subset reported using open source software (10.0\%) or field notes (20.0\%), or sharing data (12.5\%) and code (5.0\%) most of the time.

\subsubsection*{Perceived Barriers to Reproducible Research}
Aligning with the differences we observed between researcher familiarity with reproducibility enhancing research practices and researcher use of those same practices, respondents to our survey identified numerous factors contributing to a lack or reproducible research across the discipline ($\overline{b}=8.20$). 
Geographic researchers working in different subfields and with different methodological approaches each identified a similar number of barriers to reproducibility.
Qualitative geographers may be the one group that deviates from this consistent pattern. 
These researchers identified fewer barriers to reproducibility on average, but with a greater variance that left us unable to distinguish this group from any other.
To examine differences in researcher beliefs about specific factors potentially hindering reproducibility in the discipline, we divided the 12 factors we examined into three groups --- those related to the research environment, the availability of research artifacts, and study-specific characteristics. 

\begin{center}
\textbf{Insert Table \ref{tab:barriers} About Here}
\end{center}

Geographic researchers identified the incentive structure of the researcher environment as an important barrier to reproducibility in the discipline.
A majority of geographic researchers identified both the pressure to publish (71.5\%) and insufficient oversight of the research process (71.1\%) as barriers to reproducibility.
A high percentage of physical, methods-focused, quantitative, and mixed-methods researchers identified these factors as barriers. 
Qualtiative geographers were least concerned with these factors, but a majority of these researchers still identified both factors as barriers to reproducibility in the field.
Contrary to alarms raised in some of the reproducibility literature, a minority of geographic researchers (28.4\%) believe that the fabrication of data, the manipulation of research results, and similar forms of fraud are a cause of irreproducibility in the discipline.

Researchers identified the unavailability of research artifacts (e.g., data, code) as a second barrier to reproducibility, but the importance placed on different artifacts varied by subfield and methodological approach.
A higher percentage physical and methods-focused researchers identified all five of the artifacts we investigated as common barriers to reproducibility as compared to human and nature-society researchers.
A similar gap existed between qualitative researchers and those identifying as mixed-methods researchers or quantitative researchers. 
The largest differences between these groups existed in researchers beliefs about how often the availability of research protocols/code and the use of restricted data or software impacted reproducibility.
More than three-quarters of physical (77.0\%) and methods-focused (86.9\%) geographers identified the availability of protocols/code as contributing to irreproducibility, whereas less than two-thirds of human (59.7\%) and nature-society (50.0\%) identified this same factor. 
Only a large percentage of methods-focused researchers (82.0\%) identified the use of restricted data/software as a common contributor to irreproducibility in the discipline.
A similar gap existed between qualitative researchers, a minority of which identified code availability or the use of restricted data/software as contributing to irreproducibility, and quantiative and mixed-methods researchers, the majority of which saw these factors as frequent causes of irreproducibility.  

A majority of researchers identified geographic variation (71.5\%), researcher positionality (64.2\%), and chance (62.3\%) as study-specific factors limiting the reproducibility of geographic research.
Minor variations in the emphasis placed on these factors exist across subfields and approaches. 
A higher percentage of nature-society (81.8\%) and physical (80.0\%) researchers emphasized the important role the spatial variation and complexity of geographic processes can play when attempting to reproduce geographic research, but this factor was also recognized by researchers across subfields and approaches. 
A smaller percentage of physical geographers placed emphasis on the impact of researcher positionality may have on reproducibility (50.8\%) when compared to all other subfields.  
Differences between the computational environment used to conduct an original study and a reproduction attempt was generally not seen as a factor contributing to a lack of reproducibility in the discipline. 
The exception being methods-focused (68.9\%) and quantitative researchers (63.1\%\%), which aligns with the research practices used in these areas of research.


%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Attempted Reproductions}
A total of 102 of the researchers that responded to our survey (46.8\%) reported attempting a reproduction study during the past two years.
However, 23 of those reported reproduction attempts were of the researcher own work, while another 13 were in fact replications of prior work in new locations.
In the end, only \textit{$n_{a}$}=32 (14.7\%) of all respondents reported attempting to reproduce a study originally conducted by another researcher during the past two years.

This subset of 32 participants formed the basis for our second analysis of researcher practices and experiences when attempting reproductions of the work of others.
Reproduction attempts were predominantly made by geographic researchers that self-identified as working in physical geography (43.8\%) and geographic methods (37.5\%).
Respondents attempting reproductions were also focused on quantitative (68.8\%) and mixed-methods (41.2\%) approaches. 
Only eight of the researchers that attempted reproductions reported submitting any of their findings for publication.

In most instances, researchers reported some ability to access the research artifacts used in the study they were attempting to reproduce and the ability to recreate at least some of the results of that study.  
The majority of researchers that attempted reproductions (87.5\%) were able to access at least some of the data used in the original study, but few researchers (12.5\%) reported access to all of the original data.
Researchers also reported the ability to access at least some information about study procedures (68.8\%) and its computational environment (59.4\%), but limited ability to access all procedural (9.4\%) and computational information (12.5\%). 
Despite these limitations, nearly all researchers reported at least partially reproducing some results (81.3\%), but less than half reported being able to at least partially reproduce all results (21.9\%).
Few researchers were able to identically reproduce all results (9.4\%).  

How much access researchers had to study data and procedural information may impact their ability to reproduce results. 
If geographic researchers had access to at least some of the data and procedures used in the study they were attempting to reproduce, they were able to at least partially reproduce some results in nearly all cases (90.5\%), but were able at least partially reproduce all results only a fraction of the time (14.3\%).
When researchers had access to some of the data from the original study, they reported being able to at least partially reproduce all results in 6 of 24 instances. 
That success rate rose to 3 of 4 when researchers reported access to all data. 
We observed a similar increase in researcher ability to at least partially reproduce all results when researchers reported access to all procedural information. 
Having access to all procedures was linked to partial reproduction of all results in 3 of 3 instances, compared to just 6 of 19 instances when researchers could only access some procedural information. 
The small number of total reproduction of independent work reported makes it difficult to draw definitive conclusions on the relative importance of data or procedures when attempting a reproduction of geographic work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Discussion}
The results of our survey collectively suggest that while geographic researchers are aware of reproducibility 

Geographic researchers have yet to adopt a common definition of reproducibility and in some instances use the term to refer to very different types of research.  
When asked to define reproducibility, researchers provided definitions that contained different requirements for similarity across studies in terms of data, procedures, and results. 
As importantly, a minority of the geographic researchers define reproducibility as what the NASEM defines as replicability.
Interchangable use of reproducibility and replicability or an outright reversal of the meaning attached to each term has been documented across the sciences. 
Given that many geographic researchera are trained in realted disciplinary traditions and that geography has no established standard use of either term, it is likely that researchers link their definitions to those prominent in their cognate fields. 
These conflicting terminologies may lead to confusion.

Researchers that self identified as primarily using qualitative methods have a very different perspective of reproducibility. 
Many outright rejected that reproducibility was compatible with the epistemology of their subfield/approach.
Focused instead on what one qualitative researcher termed 'transferability', the desire to know that what they have discovered is relevant in other similar contexts. 
This idea is much more closely aligned with replication. 
Perhaps the way forward here is to focus on Wainwright argument that reproducibility can be though of not as a absolute standard, but a means of clarifying for others what was was done and why conclusions were drawn as they were. 
This perspective productively moves the conversation away from over focus on exactly recreating numerical results, and back to the deeper function of attempting a reproduction - assessing the argument and claims made by a researcher. 

Our survey instrument may not provide the range of option necessary to capture the practices of certain geographic subfields or methodological approaches. 
Our survey instrument was built following a review of prior instruments and our own reading of issues in the reproducibility literature. 
However, that literature over represents the sciences and also computationally intesive research. 
Indeed, the NASEM report on reproducibility and replicability quickly narrows its focus to computational reproducibility to make a report tractable. 
This emphasis necessarily carried over into our survey instrument, which may automatically lower response scores for researchers not working in these areas (e.g., qualitative, human geography). 
This reality may automatically deflate the aggregate measures of certain sub-group.
These scores do reflect their present level of understanding, but the practices may not be relevant to their methods or epistomologies. 
This fact suggests a need to more closely examine reproducibility in the context of geography's qual, NS, and human traditions.
I don't think the gap is as wide as it may seem, as qualitative respondents focused on replication of claims across space presents one possible bridge to build on. 

We just don't value or do reproductions.

\begin{itemize}
    \item Geographic researcher closely link replication and reproduction. This isn't surprising as geographic researchers often have training from other disciplines and variation in the use of the two term, and their outright reversal in meaning, is well documented. It is likely that this is the root of the conflation. This muddling of the definition has important implications ...
    \item The reproducibility literature emphasizes quantitative, particularly computational, forms of research. This emphasis necessarily carried over into our survey instrument, which may automatically lower response scores for researchers not working in these areas (e.g., qualitative, human geography). This reality may automatically deflate the aggregate measures of certain sub-group. We can use the response to Q8f as a filter to explore this issue. These scores do reflect their present level of understanding, but the practices may not be relevant to their methods or epistomologies. This fact suggests a meed to examine what reproducibility means in the context of geography's qual, NS, and human traditions.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Conclusion}

\begin{itemize}
    \item Overall story is that collectively geographers are aware of reproducible research practices, but don't have a matching amount of direct experience using those practices. There are also many perceived barriers to conducting reproducible research.
\end{itemize}

\theendnotes


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgement(s)}
We thank Tyler Hoffman for providing technical assistance in the development and execution of a set of trial queries using the Scopus API.

\section*{Funding}
This material is based on work supported by the National Science Foundation under Grant No. \textbf{BCS-2049837}.

\section*{Notes on contributor(s)}
\textbf{Kedron:} Conceptualization, Methodology, Writing - Original Draft, Writing - Review and Editing, Supervision, Project Administration, Funding Acquisition. \textbf{Holler:} Conceptualization, Methodology, Data Curation, Writing - Review and Editing, Funding Acquisition. \textbf{Bardin:} Conceptualization, Methodology, Writing - Original Draft, Writing - Review and Editing, Data Curation, Software.



\newpage
\bibliography{references}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{landscape}
\begin{table}[h]
    \centering
    \begin{threeparttable}
    \caption{Descriptive summary of survey results}
    \begin{tabular}{l c c c c c c c c c c c c}
         \hline
                    & \multicolumn{4}{1}{Subfield}  & & \multicolumn{3}{1}{Approach} & &         &   & \\
         Measure    & PH & MT & NS & HU             & & QN & MX & QL                 & & Overall & N & Missing \\
         \hline
         Definition (\textit{d}) & \begin{tabular}[c]{@{}c@{}}1.73\\ (1.11)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}2.00\\ (1.09)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}1.86\\ (1.06)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}1.84\\ (1.14)\end{tabular} & 
                        & 
                        \begin{tabular}[c]{@{}c@{}}1.91\\ (1.13)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}1.82\\ (1.04)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}1.69\\ (1.26)\end{tabular} & 
                        & 
                        \begin{tabular}[c]{@{}c@{}}1.83\\ (1.12)\end{tabular} &
                        181 &
                        37 \\
         Familiarity (\textit{f}) &  \begin{tabular}[c]{@{}c@{}}3.71\\ (0.98)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}3.97\\ (0.86)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}2.82\\ (1.76)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}2.36\\ (1.36)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}3.75\\ (0.98)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}3.46\\ (1.22)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}1.75\\ (1.30)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}3.26\\ (1.36)\end{tabular} &
                        218 &
                        0 \\
         Practices (\textit{u}) & \begin{tabular}[c]{@{}c@{}}2.06\\ (1.18)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}1.85\\ (1.38)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}1.18\\ (0.96)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}0.60\\ (0.80)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}1.63\\ (1.32)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}1.69\\ (1.24)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}0.48\\ (0.68)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}1.44\\ (1.28)\end{tabular}
                        &
                        218 &
                        0 \\         
         Barriers (\textit{b}) & \begin{tabular}[c]{@{}c@{}}8.31\\ (3.00)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}9.31\\ (2.79)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}7.44\\ (3.20)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}7.32\\ (3.57)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}8.78\\ (2.80)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}8.58\\ (2.96)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}5.97\\ (3.83)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}8.20\\ (3.24)\end{tabular} &
                        182 &
                        36 \\
        \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item Acronyms indicate: \textit{PH} Physical Geography, \textit{MT} GIScience and Methods, \textit{NS} Nature and Society, \textit{HU} Human Geography; \textit{QN} Quantitative, \textit{MX} Mixed Methods, \textit{QL} Qualitative. 
    \end{tablenotes}
    \label{tab:overview}
    \end{threeparttable}
\end{table}
\end{landscape}

%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{landscape}
\begin{table}[h]
    \centering
    \begin{threeparttable}
    \caption{Barriers to Reproducibility}
    \begin{tabular}{l c c c c c c c c c c}
         \hline
                    & \multicolumn{4}{1}{Subfield}  & & \multicolumn{3}{1}{Approach} & & \\
         Barrier    & PH & MT & NS & HU            & & QN & MX & QL              & & Overall \\
         \hline
         \textit{Research Environment}      & & & & & & & & & & \\
         Pressure to Publish                & 83.1\% & 75.0\% & 54.5\% & 61.2\% & & 77.3\% & 75.3\% & 47.5\% & & 71.5\% \\
         Insufficient oversight             & 81.6\% & 82.0\% & 63.6\% & 56.7\% & & 82.6\% & 74.2\% & 40.0\% & & 71.1\% \\
         Fraud                              & 32.3\% & 39.3\% & 9.1\%  & 22.4\% & & 29.4\% & 31.8\% & 20.0\% & & 28.4\% \\
                                            & & & & & & & & & &\\
         \textit{Artifact Availability}     & & & & & & & & & & \\
         Insufficient metadata              & 76.9\% & 78.7\% & 59.1\% & 62.6\% & & 79.4\% & 72.9\% & 45.0\% & & 80.2\% \\
         Unavailable Data                   & 89.3\% & 78.6\% & 59.1\% & 65.7\% & & 87.0\% & 81.2\% & 42.5\% & & 75.2\% \\
         Unavailable protocol/code          & 77.0\% & 86.9\% & 50.0\% & 59.7\% & & 87.0\% & 70.6\% & 37.5\% & & 71.1\% \\
         Published incomplete results       & 73.8\% & 78.7\% & 63.7\% & 56.7\% & & 78.2\% & 72.9\% & 37.2\% & & 68.4\% \\
         Use restricted data/software       & 56.9\% & 82.0\% & 40.9\% & 44.8\% & & 70.7\% & 56.5\% & 32.5\% & & 57.8\% \\
                                            & & & & & & & & & &\\
         \textit{Study Characteristics}     & & & & & & & & & & \\
         Geographic variation               & 80.0\% & 72.2\% & 81.8\% & 61.2\% & & 79.4\% & 74.1\% & 40.0\% &  & 71.5\% \\
         Researcher positionality           & 50.8\% & 70.5\% & 72.8\% & 70.2\% & & 56.5\% & 65.9\% & 80.0\% &  & 64.2\% \\
         Chance                             & 66.2\% & 65.6\% & 68.2\% & 55.2\% & & 67.4\% & 64.4\% & 52.5\% &  & 62.3\% \\
         Different computation              & 53.8\% & 68.9\% & 31.8\% & 38.8\% & & 63.1\% & 56.5\% & 25.0\% &  & 50.9\% \\
                                            & & & & & & & & & &\\
         N                                  & 65 & 61 & 22 & 67 & & 92 & 85 & 40 & & 218 \\
        \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item Cells report the percentage of respondents reporting each factor frequently, occasionally, or always contributed to a lack of reproducibility in geographic research. Acronyms indicate: \textit{PH} Physical Geography, \textit{MT} GIScience and Methods, \textit{NS} Nature and Society, \textit{HU} Human Geography; \textit{QN} Quantitative, \textit{MX} Mixed Methods, \textit{QL} Qualitative. 
    \end{tablenotes}
    \label{tab:barriers}
    \end{threeparttable}
\end{table}
\end{landscape}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\noindent PETER KEDRON is an Associate Professor in the School of Geographical Science and Urban Planning and core faculty member in the Spatial Analysis Research Center (SPARC) at Arizona State University, Tempe, AZ, 85283, US. Email: Peter.Kedron@asu.edu. His research interests include spatial analysis, geographic information science, economic geography, and the accumulation of knowledge about geographic phenomena. \\  
  
\noindent JOSEPH HOLLER is an Assistant Professor of Geography at Middlebury College, Middlebury, VT, 05753, US. Email: \\
  
\noindent SARAH BARDIN is a PhD candidate ...

\end{document}
