\documentclass[]{interact}
\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.
\usepackage{subfigure}% Support for small, `sub' figures and tables
%\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required

\usepackage{doi}
\usepackage{natbib}
%\bibliographystyle{natbib}
\bibliographystyle{chicago}
\setcitestyle{authoryear,open={(},close={)}}
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}% Bibliography support using natbib.sty

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=blue,
}

\usepackage{titlesec}
\titleformat*{\section}{\Large\bfseries}
\titleformat*{\subsection}{\large\bfseries}

\usepackage{endnotes}
\let\footnote=\endnote
\usepackage{etoolbox}
\patchcmd{\enoteformat}{1.8em}{0pt}{}{}

\theoremstyle{plain}% Theorem-like structures provided by amsthm.sty
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}

\usepackage{tabularx}
\usepackage{booktabs,caption}
\usepackage{threeparttable}

\usepackage{lscape}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\articletype{DRAFT MANUSCRIPT}

\title{Reproducible Research Practices and Barriers to Reproducible Research in Geography: Insights from a Survey}

\author{
\name{Peter Kedron\textsuperscript{a,b}\thanks{CONTACT Peter Kedron. Email: Peter.Kedron@asu.edu}, Joseph Holler\textsuperscript{c}, and Sarah Bardin\textsuperscript{a,b}}
\affil{\textsuperscript{a}School of Geographical Sciences and Urban Planning, Arizona State University, Tempe, Arizona, USA; \textsuperscript{b}Spatial Analysis Research Center (SPARC), Arizona State University, Tempe, Arizona, USA; \textsuperscript{c}Department of Geography, Middlebury College, Middlebury, Vermont, USA}
}

\maketitle

\begin{abstract}
The number of reproduction and replication studies undertaken across the sciences continues to rise, however such studies have not yet become commonplace in geography. 
Existing attempts to reproduce geographic research suggest that many studies cannot be fully reproduced, or are simply missing components needed to attempt a reproduction. 
Despite this suggestive evidence, we have not yet systematically assessed geographers' perceptions of reproducibility or the use of reproducible research practices across the discipline's diverse research traditions; nor have we identified the factors that keep geographers from conducting more reproduction studies.
We address each of these needs by surveying active geographic researchers selected using probability sampling techniques from a rigorously constructed sampling frame.
We identify a clear division in perceptions of reproducibility among geographic subfields. 
We also find varying levels of familiarity with reproducible research practices and a perceived lack of incentives to attempt and publish reproduction studies. 
Despite many barriers to reproducibility and divisions between subfields, we also find common foundations for examining and expanding reproducibility in the field.
These include interest in publishing transparent and reproducible methods, and in reproducing other researchers' studies for a variety of motivations including learning, assessing the internal validity of a study, or extending prior work.


\end{abstract}

\begin{keywords}
Reproducible Research, Epistemology, Geographic Research Methods
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{Introduction}
Reproducible research publicly discloses the evidence used to support claims made in prior work, facilitates the independent verification of those claims, and enables the extension of that work by the broader research community \citep{earp2015, nosek2012scientific, Schmidt2009}.
While reproducibility is not a guarantee of scientific or practical usefulness, it does provide a strong basis for the collective evaluation of ideas.
Nonetheless, systematic reviews of published research papers reveal a lack reproducibility \citep{moraila2014measuring, iqbal2016reproducible}; and furthermore, attempts to reproduce published research papers frequently fail \citep{raghupathi2022reproducibility, chang2015economics}.
Previous studies consistently link the irreproducibility of research to inadequate record keeping, opaque reporting, the inaccessibility of research components, and a lack of incentives to share research details or to attempt reproduction studies \citep{ranstam2000fraud, anderson2007normative, NASEM2019}.
Surveys of researchers find that few researchers are attempting to independently reproduce the work of others \citep{baker20161, boulbes2018survey}.
At the same time, a sizable portion of survey respondents report knowing of instances in which researchers engaged in questionable or biased research practices tied to the publication of false positive results and low reproducibility rates \citep{fanelli2009many, fraser2018questionable}.

Despite these concerns, the available literature currently provides insufficient evidence to conclusively evaluate the reproducibility of research generally, or disciplinary research specifically.
This knowledge gap exists because researchers conducting reproducibility assessments and surveys of researcher practices have not defined clear target populations for inference, or used probability sampling techniques structured by well-defined sampling frames that support generalization \citep{NASEM2019}.
Attempts to reproduce published studies typically focus on recreating the results of a small number of studies selected based on topical interest or researcher familiarity \citep{camerer2016evaluating, camerer2018evaluating, open2015estimating}. 
Attempts to catalog reproducible research practices have drawn samples of research papers from archives such as conference paper series, specific journals, or disciplinary repositories \citep{byrne_2017, gundersen2018state, stodden2016enhancing, stodden2018enabling}.
Surveys of researchers have drawn samples from authors in specific journals, members of professional associations, or conference attendees \citep{baker20161}.
All of these sampling frames are convenient, but are not guaranteed to be representative of researchers and research activity in a particular field. 
Surveys have also commonly failed to systematically report the methodological details (e.g., response rate) needed to assess and address potential bias in survey response, which further complicates their evaluation and use. 
Finally, reproducibility assessments have focused on evaluating the computational components of past studies, such as data and code availability, rather than all aspects of research design and execution.

Reproducibility surveys and reproduction attempts in the geographic literature face these same challenges. 
The few available reproducibility surveys in the discipline have relied on convenience samples drawn from specialist conferences and have only focused on computationally-intensive forms of geographic research \citep{balz2020reproducibility, konkol2019, ostermann2017}. 
The small number of published attempts to reproduce geographic research have similarly focused on the computational reproducibility of conference papers \citep{Nust-AGILE_2018, Nust_AGILE_2020, Nust_AGILE_2021, Nust_AGILE_2022, ostermann2021} or on specific topics such as COVID-19 \citep{Holler2023disability, Kedron2022dimaggio, Kedron2023Beyond, paez2022reproducibility}. 
More recent reproduction attempts by \citet{Kedron2023Beyond} show that the factors hindering the recreation of results and the evaluation of claims likely extend beyond computation into the conceptualization and design of geographic research.

Geographers continue to debate the role of reproduction studies in the discipline \citep{brunsdon2016, goodchild2021Annals, kedron2022replication, kedron2021IJGIS, singleton2016, sui2021, Wainwright2021}, examine the reproducibility of individual studies \citep{Nust_AGILE_2022, ostermann2021}, and build the infrastructure needed to support reproducible research \citep{nust2019, wilson2021, yin2019cybergis, Kedron_Holler_Bardin_Hilgendorf_2022}. 
And yet, we still have not systematically assessed the use of reproducible research practices across the discipline's diverse research traditions, or identified the factors that have hindered geographers from adopting reproducible research practices and conducting reproduction studies. 
Without answers to these questions it is unclear what actions geographers should take if they wish to improve the reproducibility of work in the discipline.

To address this gap in our collective knowledge, we surveyed geographic researchers about their understanding of reproducibility, perception of reproducibility in their subfields, familiarity and use of reproducible research practices, and barriers to reproducibility.
To support generalization, we design a sampling frame to capture researchers from across disciplinary subfields and methodological approaches, and draw survey participants from that frame using a probability sampling scheme.
In the remainder of this paper, we first present the design of our survey, sampling strategy, and analytical approach. 
We then present the results of our survey, first focusing on researcher perceptions and use of reproducible research practices, and then analyzing researcher experiences attempting to reproduce prior work. 
Finally, we discuss the implications of our survey results and limitation of our work, and conclude by proposing where the geography may go from here and how the discipline may contribute to reproducibility across the sciences. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Data and Methods}
Complete documentation of the procedures, survey instrument, and other materials used in this study are available through the Survey of Reproducibility in Geographic Research project (\citet{Kedron_Holler_Bardin_Hilgendorf_2022} - \url{https://osf.io/5yeq8/}) hosted by the Open Science Framework (OSF).
The OSF project connects to a GitHub repository which hosts the anonymized dataset and code used to create all results and supplemental materials along with a complete history of their development. 
All of the results presented in this paper can be independently reproduced using the materials in that repository.
We encourage interested readers to critically evaluate and build on these materials.
Before the start of data collection, we registered a preanalysis plan for the survey with OSF Registries (\citet{Kedron_Survey_PAP} - \url{https://osf.io/6zjcp}). 
The survey was conducted under the approval and supervision of the Arizona State Institutional Review Board - \textit{STUDY00014232}.

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Sampling Frame}
Our target population of interest is researchers who have recently published in the field of geography. 
We followed a 4-step procedure to create a sampling frame for our survey that captures this diverse population of researchers and the approaches they use when studying geography. 

First, beginning at the publication level, we identified journals indexed as either geography or physical geography by the Web of Science's Journal Citation Reports that also had a 5-year impact factor greater than 1.5.
From those journals, we created a database of all articles published between 2017 and 2021.  

Second, we used Arizona State University's institutional subscription to the \citet{Scopus} to extract journal information (e.g., subject area), article information (e.g., citation counts), and author information (e.g., corresponding status) for each publication. 
Because our intention was to capture individuals actively publishing new geographic research, we retained publications indexed by Scopus as \textit{document type = ``Article''} and removed all other publication types (e.g., editorials) from our article database. 
We also removed articles with missing authorship information. 

Third, we created a list of researchers and their published articles, focusing on corresponding authors for two reasons.
First, corresponding authorship is one indicator of the level of involvement an individual had in a given work. 
While imperfect, it was the best available indicator in the Scopus database as across journals there is no commonly adopted policy for declarations of author work (e.g., CRediT Statements).
Second, Scopus maintains email contact information for all corresponding authors, which gave us a means of contacting researchers in our sampling frame.
Scopus also maintains a unique identifier for each author (\textit{author-id}) across time, which allowed us to identify authors across publications. 

Fourth, we constructed a sampling frame of unique researchers and their most recent email contact information. 
We determined uniqueness by grouping researchers by their \textit{author-id}, and we determined the most recent contact information by selecting records associated with the most recent year of publication. 
For 383 researchers who had two or more distinct emails in the latest year of publication, we removed non-institutional personal email addresses and then selected one of the remaining institutional email address.

Applying these criteria yielded a sampling frame of 29,828 researchers. 
On average, these authors published 2.7 articles in geography journals meeting our criteria between 2017 and 2021. 
Roughly one-third (33.0\%) were most recently a corresponding author for an article published in a general geography journal. 
A similar proportion (32.0\%) were most recently a corresponding author for an article published in an earth sciences journal, and smaller proportions published in the social sciences and cultural geography (20.0\% and 16.0\%, respectively).

\subsection*{Survey Instrument}
The survey first established eligibility based on age and geographic research activity in the past five years and asked researchers to report their primary subfield and methodology.
We asked each participant to assess their familiarity with the term ``reproducibility'' and to provide their own definition. 
We then provided a definition based on the \citet{NASEM2019} to establish a common understanding of reproducibility for the remainder of the survey.
Remaining questions assessed familiarity and use of reproducible research practices (22 questions), perceptions of the reproducibility of geographic research (2 questions), and beliefs about reproducibility with regards to its significance (17 questions) and barriers (13 questions).
For researchers who reported attempting reproductions, we asked them to elaborate on their motivations and outcomes (9 questions).

We developed the survey questions following a review of prior reproducibility surveys \citep[e.g.,][]{fanelli2009many,baker20161, konkol2019} and our own reading of recurring issues in the reproducibility literature. 
We pilot tested the survey instrument with \textit{n}=19 graduate students and geography faculty with differing levels of experience, disciplinary subfields, and methodological background. 
After pilot testing, we removed these individuals from our sampling frame to ensure they would not be included in our final sample.

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Data Collection}
We used a digital form of the Tailored Design Method \citep{dillman2014internet} to survey geographic researchers between May 17 and June 10, 2022.
A simple random sample of 2,000 researchers was drawn without replacement from our sampling frame, and those researchers were invited via email to participate in the online survey. 
Researchers received their initial invitation on May 17, 2022. 
Two reminder emails were sent to researchers that had not yet completed the survey on May 26 and May 31, 2022.

The online survey was administered through \href{https://www.qualtrics.com/}{Qualtrics}. 
Participation in the survey was entirely voluntary. 
Each researcher that opted to participate in the survey was provided with IRB-approved consent documentation and linked to the internet survey instrument. 
Participants were also given the option to provide an email address for eligibility for one of three  prizes of 90 USD, selected randomly after the data collection period.
Participating researchers had the option to exit and re-enter the survey and were also able to review and change their answers using a back button as they progressed through the survey.
At the end of the data collection period, responses were checked for completeness and coded using the reporting standards of the American Association For Public Opinion Research \citep{aaporstandards}.
Responses were downloaded from Qualtrics, anonymized, and stored in a public de-identified database in the research compendium.

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Analytical Approach}

We conducted two statistical analyses of the survey responses.
First, we analyzed researcher perspectives on reproducibility by coding and calculating summary variables and statistics for three themes: how geographic researchers define reproducibility, familiarity and experience with reproducible research practices, and perceived barriers to reproducibility.
Second, we analyzed researchers' experiences with reproducing prior studies including their motivations and experience of successes and barriers.

\subsubsection*{Analyzing Researcher Perspectives on Reproducibility}

\noindent For our first set of analyses, we examined the full set of survey responses.

\textit{Defining Reproducibility:} 
We coded participants' qualitative definitions of ``reproducibility'' following two procedures.
First, we measured the similarity of each provided definition to the definition adopted by the \citet{NASEM2019}. 
The NASEM defines reproducible research as having four characteristics --- same data, same procedure, same results, and same conditions.
To make this comparison, each of the authors independently coded each respondent definition for the presence/absence of each of the four characteristics included in the NASEM definition.
Disagreements in the assignment of codes were resolved through discussion between the three authors.
We created an aggregate measure of definition similarity for the final coded response for each participant by counting the presence of each NASEM definition characteristic, resulting in a measure with the domain [0, 4].

Second, we coded each definition to one of four motivations for ensuring the reproducibility of a study: (i) to facilitate the assessment of prior work, (ii) to assess experimental research, (iii) to improve transparency and facilitate further extension of work, and (iv) to improve the transparency and consistency of data collection.
We derived this coding from common themes in the responses and our own reading of the reproducibility literature.
As above, each definition was independently coded by each author before code assignments across authors were compared with disagreements resolved through discussion.

\textit{Familiarity and Experience:} We measured participant familiarity and experience with five reproducibility enhancing research practices: the adoption of open-source software, research notebooks, data sharing, code sharing, and research plan preregistration. 
We created a measure of familiarity by counting the variety of practices a researcher was `somewhat' or `very' familiar with.
We measured experience by counting the variety of practices a researcher reported using `most of the time' or `always' when conducting their own research. 
Both measures have domains [0, 5]. 

\textit{Barriers:} Finally, we asked asked respondents to identify how frequently they believe 12 different factors contributed to a lack of reproducibility in their subfield. 
From those responses, we created a measure of perceived barriers by counting the number of factors a researcher believes `frequently' or `occasionally' contributed to irreproducibility, resulting in a measure with domain [0, 12].

To examine variation in researcher understanding of reproducibility, we cross tabulated responses to individual questions and these summary measures by disciplinary subfields and methodological traditions. 
We then compared response frequencies across subgroups.

%%%%%%%
\subsubsection*{Analyzing Researcher Reproduction Attempts}
\noindent For our second set of analyses, we examined only the responses of researchers who reported attempting a reproduction in the past two years to understand what motivated reproduction attempts, how successful those attempts were, and what factors hindered success.

\textit{Motivations:} To assess what motivated researchers to attempt reproductions, two of the authors independently assigned participants text-based response to groups with the following motivations: i) verify/check published research, ii) learn from published research for extension or teaching, iii) internally check their own research to verify their work and/or increase the transparency of their work, and iv) replicate a study with new data.
We identified disagreements in motivation assignments, and the third author again resolved all disputes in consultation with the two assigning authors.
We selected groups i) and ii) for further analysis of reproduction attempts, as these motivations most closely matched the NASEM definition of reproducibility. 

\textit{Success and Barriers:} To analyze participant success, we created statistical summaries for a series of questions that asked researchers to identify whether they were able to partially or completely recreate some or all results of the target study. 
We similarly analyzed barriers to the reproduction of results by creating statistical summaries of participants' ability to access key study artifacts (e.g., data, procedural information, and code).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Results}
A total of \textit{n}=218 of the authors we contacted completed the online survey with information sufficient for analysis. 
The contact rate for the survey was 13.9 percent and the cooperation rate was 78.7 percent, yielding an overall response rate of 10.9 percent. 
The refusal rate was 2.9 percent\endnote{All outcome rates are reported using \citet{aaporstandards} standards. 
The outcome rates used were - response rate 2, cooperation rate 2, refusal rate 1, and contact rate 1.}.
Another \textit{n}=40 authors started the survey, but did not complete enough of the survey to be included in the analysis.

Respondents were predominantly male (65.1\%) and between the ages of 35 and 55 (62.4\%). 
The majority of respondents were academics, and they were balanced across career levels from graduate students to full professors with no one career level comprising more that 30 percent of the sample.
Respondents identified with each of the four major disciplinary subfields---physical geography (29.8\%), geographic methods and GIScience (28.0\%), nature and society (10.1\%), and human geography (30.7\%); and three major methodological approaches---quantitative (42.2\%), mixed methods (39.0\%), and qualitative (18.3\%).

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Researcher Perspectives on Reproducibility}
Reproducibility is on the minds of geographic researchers.
Nearly all researchers reported being at least somewhat familiar with the term reproducibility (89.0\%), with half reporting being very familiar with the term (53.6\%).
More pointedly, the majority of survey respondents reported thinking about the reproducibility of their own research (80.7\%), discussing reproducibility with a colleague (70.6\%), and questioning the reproducibility of published work (57.3\%) in the past two years. 
More than half of the researchers we surveyed (52.8\%) also reported considering reproducibility while peer reviewing a grant proposal or publication during the same time frame. 
However, researchers estimated that only 50.6 percent of the results published in the discipline were reproducible, albeit with a large standard deviation of 24.7 percent that suggests a great deal of uncertainty about the true value. 
Few respondents reported attempting to reproduce the work of other researchers (14.7\%) with fewer still attempting to publish those reproduction studies (6.8\%). 

Strikingly, only 58 percent of respondents believe reproducibility is compatible with the epistemologies used in their disciplinary subfield.
An additional 28 percent of respondents reported that reproducibility was incompatible with the epistemologies of their subfields, and  12 percent of respondents indicated that they did not know whether reproducibility was epistemologically compatible. 
About half of the respondents specializing in human geography (49\%) and nature and society (50\%) indicated that reproducibility was incompatible with the epistomologies of their subfields. 
Respondents conducting primarily qualitative research were similarly skeptical of the epistemological role of reproducibility in their subfield.
75 percent of qualitative researchers indicated that reproducibility was incompatible with the epistemologies of their subfield.

Table \ref{tab:overview} summarizes how researchers define reproducibility, their familiarity with reproducible research practices and experience using them, and the factors they see as barriers to reproducibility in geography. 
We present the mean and standard deviation for the overall sample and for each subfield and methodological approach.
In aggregate, the data reveal consistent trends in definition, familiarity, experience, and barriers of reproducibility between the sub-disciplines and methodological approaches. 
Respondents that self-identified as specializing in physical geography and geographic methods consistently report higher indicators than those working in nature and society and human geography.
Similarly, respondents that identified as primarily using quantitative and mixed methods approaches consistently report higher indicators than those using qualitative methods.
The following sub-sections present detailed results for each of these topics, highlighting the principal sources of difference between subfields and methodological approaches.

\begin{center}
\textbf{Insert Table \ref{tab:overview} About Here}
\end{center}


%%%%%%%%%%%
\subsubsection*{Definitions and Importance of Reproducibility}
A total of 181 (83.0\%) of our survey respondents provided an interpretable definition of reproducibility.  
Geographic researchers provided definitions of reproducibility that explicitly included an average of 1.83 of the four characteristics from the definition adopted by the NASEM.
The availability and use of the same research procedures (80.7\%) and results (74.0\%) were the characteristics of reproducibility most frequently identified by researchers. 
Less than half of respondents explicitly included use of the same data (38.1\%) or the need to work in the same context (17.7\%) in their definitions. 
The pattern of similarity to the NASEM definition and each of its components was consistent across subfields and methodological approaches, with a slightly greater emphasis on data and procedural availability amongst quantitative and geographic methods and GIS researchers.
The lower inclusion of data and context in definitions may be explained by researchers conceptualizing reproducibility as the formal NASEM definition of replicability, which emphasizes the testing of similar questions and procedures in new contexts with new data. 
For example, one respondent defines reproducibility as, ``the extent to which the research design can be replicated in different geographical contexts."
We observed this alternative definition of reproducibility in 20.4\% of respondents' definitions.

Researchers' definitions of reproducibility were primarily connected to two epistemic functions.
Just over half of respondents (52.5\%) defined reproducibility as a means of assessing prior work for errors or inconsistencies through comparison of original results to results from an attempted reproduction.
These comparisons ranged from rigid bitwise quantitative interpretations, as in the ``ability to re-generate exactly the results published based on the data and code provided by the authors", to more flexible interpretations in which ``other researchers could use the same or similar methodology without great difficulty and, given similar data, arrive at comparable results."
Responses included definitions with a focus on experimental science, as in ``an ability to produce consistent results when an experiment is repeated!"

Nearly all other researchers (40.9\%) tied reproducibility to the need for transparency in research so that others could independently expand upon past studies.
For example a quantitative geographer stated: ``The methods should provide sufficient information to be able to reproduce the results. In quantitative science this should, at minimum, provide all the equations and algorithms used for any calculation. In the interest of increasing transparency in science, the practice of sharing the code should be encouraged."
For others, open science did not necessarily need to result in identical results, reflected in this definition: ``As a qualitative researcher doing in-depth case study research, my studies can not be perfectly reproduced. But reproducibility sits in the openness about methods and data collection practices, as well as critical reflection about strengths and weaknesses of my research. When we write about those things in the methods section in our papers and theses, their reproducibility is increased." 

The remainder of responses (6.6\%) emphasized repeatable or reliable data observation over all other dimensions of reproducibility.
For example, ``As a historical geographer, working with qualitative research methods, I understand reproducibility more in terms of sources than of methods. I see reproducible research as being that which makes clear the origin and location of its data."
A physical geographer similarly emphasized data observations: ``Data/observations of some variable can be recovered repeatedly by different observers/methods."  

Responses to related Likert questions from the full survey sample (\textit{n}=218) support the results from the subsample of 181 qualitative definitions analyzed above.
A majority of researchers identified reproducibility as important for validating (75.2\%) and establishing the credibility (72.5\%) of research.  
Respondents also saw reproducing studies as important to reducing the presence of persistent errors in the discipline (77.5\%) and to increasing trust in research findings (78.5\%).
In parallel with the need for openness and transparency in science, most respondents agreed with the importance of reproducibility for research efficiency (63.3\%), communication with academics (68.8\%) and practitioners (64.7\%), and training students (75.7\%).

Despite wide recognition of reproducibility as epistemically important, respondents were cautious about drawing conclusions from a single study or reproduction attempt. 
Only half of the respondents (50.9\%) agreed that when researchers don't share their data they have less trust in a study.
A smaller percentage (41.7\%) agreed that inability to reproduce a result detracts from the validity of a study, and an even smaller minority agreed that such inability implies that the result is false (26.2\%).

Qualitative researchers identified reproducibility as playing a much smaller epistemic role compared to the discipline as a whole. 
A small percentage of qualitative researchers agreed that reproducibility is important for validating research (25.0\%) or establishing its credibility (20.0\%).
Qualitative geographers similarly placed less emphasis on reproducibility as a means of increasing the accessibility and extensibility of research.
Few qualitative researchers had less trust in a study when researchers did not share their data (27.5\%), or saw reproducibility as important for sharing research with academics (27.5\%).

The data from our sample of researchers has shown broad recognition of reproducibility and its importance in geography with three caveats---confusion between reproducibility and replicability, different perspectives from researchers using qualitative methods, and caution about judging the trustworthiness or validity of published research based on its reproducibility. 
In this context, are individual researchers aware of the research practices needed to enhance reproducibility, and have these practices already been adopted for use in research?

%%%%%%%%%%%
\subsubsection*{Familiarity and Experience with Reproducible Research Practices}
Geographic researchers were familiar with an average of 3.26 different reproducible research practices, but only reported experience using an average of 1.44 of these practices in their own work.
Table \ref{tab:familar-use} presents researcher familiarity and use of five different reproducible research practices.
More than half of all researchers reported familiarity with with data sharing (86.7\%), open source software (85.3\%), field and lab notebooks (67.0\%), and code sharing (59.2\%).
However, a far smaller number of researchers reported using these ``familiar" practices regularly in their own work. 
Less than half of the researchers surveyed reported sharing their data (44.5\%), using open source software (38.1\%), using field or lab notebooks to record their work (40.0\%), or sharing their code (18.8\%) most or all of the time. 
Only a small subset of researchers reported familiarity with the pre-registration of research designs and protocols (27.5\%), or regular use of this practice (2.7\%).

\begin{center}
\textbf{Insert Table \ref{tab:familar-use} About Here}
\end{center}

Researcher familiarity and use of reproducible research practices varied by disciplinary subfield and methodological approach. 
Researchers that identified as physical geographers or methodologists and GIScientists reported being familiar with one to two more reproducible research practices than human geographers and those focused on nature and society.
Researcher practices similarly diverged by subfield, but no subset of researchers reported using on average more than two of these practices regularly in their work.  
Quantitative and mixed-methods researchers reported familiarity with and use of an average of two more reproducible research practices when compared to qualitative researchers.

Differences in researcher familiarity and use of specific reproducible research practices across subfields and approaches was greatest for practices more typical of quantitative workflows. 
When compared to qualitative researchers, quantitative and mixed-methods researchers reported greater familiarity with all reproducible research practices.
For example, just 12.5 percent of qualitative researchers reported familiarity with code sharing, while 81.5 percent of quantitative and 57.7 percent of mixed-methods researchers reported familiarity with the same practice.
However, even among quantitiative and mixed-methods researchers, familiarity with reproducible research practices did not translate into regular use of those practices.
Just over half of quantitative and mixed-methods researchers reported regularly sharing their data.
Less than a quarter of these same researchers reported regularly sharing their code. 
These rates were lower for qualitative geographers, of which only a small subset reported using open source software or field notes, or sharing data and code most of the time or always.

\subsubsection*{Perceived Barriers to Reproducible Research}
Aligning with the differences we observed between researcher familiarity with reproducibility enhancing research practices and researcher use of those same practices, respondents to our survey identified an average of 8.20 factors contributing to a lack or reproducible research. 
Geographic researchers working in different subfields and with different methodological approaches each identified a similar number of barriers to reproducibility, as shown in Table \ref{tab:barriers}.
Qualitative geographers may be the one group that deviates from this consistent pattern. 
These researchers identified fewer barriers to reproducibility on average, but with a greater variance that left us unable to distinguish this group from any other.
To examine differences in the specific factors researchers believe hinder reproducibility in the discipline, we divided the 12 factors into three groups---those related to the research environment, the availability of research artifacts, and study-specific characteristics. 

\begin{center}
\textbf{Insert Table \ref{tab:barriers} About Here}
\end{center}

Geographic researchers identified the incentive structure of the researcher environment as an important barrier to reproducibility.
A majority of geographic researchers identified both the pressure to publish original research (71.5\%) and insufficient oversight of the research process (71.1\%) as barriers.
A minority of qualitative researchers identified both factors as barriers, but a majority of researchers in all other approaches and subfields identified both factors as barriers to reproducibility.
Physical, methods-focused, and quantitative researchers identified these factors as barriers in higher numbers. 
A minority of geographic researchers (28.4\%) believe that the fabrication of data, the manipulation of research results, and similar forms of fraud are a cause of irreproducibility in the discipline.
This percentage is consistent with concerning results from large surveys and meta-analyses of research on scientific fraud across other scientific disciplines \citep{fanelli2009many, baker20161}.

Researchers identified the unavailability of research artifacts (e.g., data) as a second barrier to reproducibility, but the importance placed on different artifacts varied by subfield and methodological approach.
A higher percentage physical and methods-focused researchers identified all five of the artifacts we investigated as common barriers to reproducibility as compared to human and nature-society researchers.
The largest differences between these groups existed in researchers' beliefs about how often the availability of research protocols/code and the use of restricted data or software impacted reproducibility.
A similar gap existed between qualitative researchers and mixed-methods or quantitative researchers with regards to identifying code availability or the use of restricted data/software as contributing to irreproducibility.

A majority of researchers identified the complexity and variability of a system (71.5\%), researcher positionality (64.2\%), and chance (62.3\%) as study-specific factors limiting the reproducibility of geographic research.
Minor variations in the emphasis placed on these factors exist across subfields and approaches. 
A higher percentage of nature-society and physical researchers emphasized the important role that spatial variation and complexity of geographic processes can play when attempting to reproduce geographic research, but this factor was also recognized by researchers across subfields and approaches. 
A smaller percentage of physical geographers placed emphasis on the impact of researcher positionality may have on reproducibility when compared to all other subfields.  
Differences between the computational environment (computer hardware and software) used to conduct an original study and a reproduction attempt was generally not seen as a factor contributing to a lack of reproducibility in the discipline. 
Of all subgroups, only a majority of methods-focused and quantitative researchers were concerned with computational environments, reflecting research practices used in their areas of research.


%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Attempted Reproductions}
A total of 102 of the researchers that responded to our survey (46.8\%) reported attempting a reproduction study during the past two years.
However, 23 of those researchers were reproducing their own research results, while another 13 were in fact replicating prior studies in new locations.
In the end, only \textit{n}=32 (14.7\%) of all respondents reported attempting to reproduce a study originally conducted by another researcher during the past two years.

This subset of 32 participants formed the basis for our analysis of researcher practices and experiences when attempting reproductions of the work of others.
Reproduction attempts were predominantly made by geographic researchers that self-identified with the physical geography (43.8\%) or geographic methods and GIS (37.5\%) subfields.
Respondents attempting reproductions were also focused on quantitative (68.8\%) and mixed-methods (41.2\%) approaches. 
Only eight of the researchers that attempted reproductions reported submitting any of their findings for publication.

Most of the 32 researchers that attempted to reproduce a prior study reported at least some success in accessing data and procedures and in reproducing the prior study results.   
The majority of researchers (87.5\%) were able to access some of the data used in the original study, but few researchers (12.5\%) reported access to all of the original data.
Researchers also reported the ability to access at least some information about the study procedures (68.8\%) and computational environment (59.4\%), but limited ability to access all procedural (9.4\%) and computational environment information (12.5\%). 

Reproduction attempts may produce results for comparison to \textit{some} or \textit{all} of the results in a prior study.
A reproduction may be \textit{identical} by finding the exact same results, or may be \textit{partial} by finding slightly different results that still support the same conclusions.
Nearly all researchers reported at least partially reproducing some results (81.3\%), but only seven (21.9\%) reported being able to at least partially reproduce all results.
Only three researchers (9.4\%) were able to identically reproduce all results.  

Access to prior study data and procedural information appears to impact the ability to reproduce prior study results. 
When researchers had access to some of the data from the original study, they reported being able to at least partially reproduce all results in 6 of 24 instances. 
That success rate rose to 3 of 4 when researchers reported access to all data. 
Procedural information and code appears to matter as much as data.
When researchers had access to some of the procedural information from the original study, they reported being able to at least partially reproduce all results in 6 of 19 instances.
That success rate rose to 3 of 3 when researchers reported access to all procedural information and all code.

The small number of reproduction study attempts reported in the survey results make it difficult to draw broad conclusions.
However, the results are internally consistent and intuitively support the importance of available data and procedures for the reproducibility of geographic research.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Discussion and Limitations}
Our survey results indicate that geographic researchers are aware of reproducibility and reproducible research practices, but have yet to incorporate many of those practices into their own work. 
We found that few researchers attempt to independently reproduce the work of others, or to publish the reproduction attempts they do undertake.
In alignment with the broader reproducibility literature, geographic researchers identify the lack of methodological transparency and the unavailability of data and procedural information as key barriers to reproducibility in the discipline. 
Our results also suggest the need to change the culture of research, publication, and promotion within the discipline, to one that recognizes, values, and rewards attempting and publishing reproduction studies and original researcher that is reproducible.  
On the whole, awareness of reproducible research practices and the infrastructure to attempt reproductions and publish reproducible work exist within the discipline, but geographers have yet to make either a regular part of disciplinary practice. 

Our findings also suggest that geographic researchers do not share a single definition of reproducibility. 
While researchers share beliefs about the epistemological functions of independent reproductions, they provide definitions that contain different requirements for similarity across studies in terms of data, procedures, results, and context.  
Moreover, a subset of researchers define reproducibility as what the \citet{NASEM2019} defines as replicability---the ability to obtain consistent results across studies designed to answer the same question, each of which has obtained its own data.
The interchangeable use of reproducibility and replicability, or the outright reversal of definitions we observed in our sample, has also been documented across the sciences \citep{barba2018terminologies, plesser2018reproducibility}. 
Given that geography has no established standard use of either term and that many geographic researchers are also trained in other disciplines, it is likely that researchers at least partially inform their definition of reproducibility using concepts prominent in their cognate fields.

More broadly, the variation in terminology we observed is important for at least two reasons. 
First, variation in geographic researchers' understanding of reproducibility reflects the discipline's diverse traditions and ways of knowing. 
Acknowledging this diversity as a strength of the discipline, productive discussions about reproducibility should consider how reproducible research practice fits into different traditions and what common understanding exists across traditions. 
Second, if researchers lock into an ontological debate about the definition of reproducibility, the community may hinder a more productive discussion about the epistemological role independent reproductions and open science practices can or should play in the discipline. 

Our findings point to potentially productive pathways for such a discussion. 
For example, qualitative geographers are subset of respondents that most frequently diverged from other respondents using other approaches and working in other subfields.
This subset of respondents had much less familiarity and use of reproducible research practices and more frequently disagreed that reproducibility was compatible with their epistemological approach.
These differences may also explain their lower rates of reporting barriers to reproducibility. 
However, our qualitative respondents did consistently value particular epistemic functions of reproducibility at higher rates than their disagreement with reproducibility on epistemological grounds suggests.
This contradiction suggests that qualitative methodologists and reproducibility researcher have yet to meaningfully engage despite sharing some common values.
One commonly held value that could serve as a platform for such an engagement is the shared belief in the importance of transparency and precise communication in research.

Qualitative researchers may also have much to contribute to the reproducibility literature owing to their unique perspective and different approach to research.  
For example, qualitative researchers are more concerned that any other group that research positionality is a barrier to reproducibility, but other groups also recognize the impact researcher position and experience can have on research.
Perhaps one way forward is to initiate a conversation that highlights how reproducibility is not an absolute standard or determinant of research quality, but instead a means of clarifying for others what was was done in a study and why conclusions were drawn as they were.
Even if a researcher believes positionality influences data collection and interpretation, using reproducible research practices to control all variables of research design \textit{except} researcher position may help convey that position and its impact on study results.
To our knowledge, there is little explicit discussion in the reproducibility literature about how researcher positionality can or should be recorded and conveyed to other researchers. 
It would also be interesting to know whether certain elements of researcher positions appear stable across researchers and contexts.
Such work could also move the conversation about reproducibility away from a current focus on the exact recreation of numerical results, and back to the practice's deeper function---independently assessing the claims of prior research. 

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Limitations}
To our knowledge, our work is the first systematic attempt to survey geographic researchers about reproducibility. 
To draw a reliable and generalizable understanding of this issue, we systematically developed robust sampling frame representative of all geographic researchers. 
Ideally, we would stratify this set of potential respondents into meaningful subgroups based their knowledge of reproducibility, and then randomly draw participants from these subgroups. 
If our resulting sample was imbalanced, we would then use a post-stratification procedure balance the response.   

We could not follow this approach for two reasons. 
First, meaningful stratification and post-stratification require knowledge of what predicts differences in response.
Given the currently limited understanding of reproducibility within geography, we can only speculate about the researcher characteristics predictive of different levels of familiarity and experience with reproducible research, e.g. subfield, methodological approach, or career position.
We do not have the knowledge needed to identify reliable predictors.
However, our survey lays an initial foundation for examining reproducibility in subsequent studies by providing the first discipline-wide measurement of predicitive researcher characteristics. 
Second, meaningful stratification and post-stratification require a population wide census of key predictors of reproducibility.
We are not aware of any census of geographic researchers that contain this data, and believe that conducting such a census would be difficult given the diversity of the field and the fuzzy boundaries between the discipline's subfields.
Given the limitations to stratifying or balancing a survey on reproducibility in geography, our study should be viewed as an exploratory analysis with random sampling and a transparent, reproducible methodology for sample frame construction.

Absent stratification, we have taken steps to reduce several forms of potential bias in our survey. 
We have worked to eliminate exclusion bias by including in our sampling frame all researchers publishing as corresponding authors in any wide range of geography journals over a five year period.
While we cannot eliminate the possibility of self-selection bias from our survey, we attempted to quantify potential self-selection by calculating and comparing the completion rates across subfields and approaches. 
Completion rates for all subfields were between 84 and 87 percent, except slightly higher rates for geographic methods and GIS researchers (96.8\%).
Completion rates were 84.2 percent for mixed methods, 87.0 percent for qualitative methods, and 91.1 percent for quantitative methods. 
These values suggest that self-selection was not a significant issue.
Finally, we attempted to mitigate the potential for questionnaire bias, which could be caused by partially basing our survey instrument on prior studies that over-represent perspectives from the computational and experimental sciences.
To address this concern, we incorporated into our survey questions from a parallel review of the reproducibility literature available within geography and a review of critiques of positivist science made by social scientists and human geographers. 
We also included space for text-based qualitative responses in each survey theme, and pilot tested out instrument with a diverse set of geographers.

%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Conclusion}
In this study, we have provided the first systematic survey of the use of reproducible research practices across geography's diverse research traditions. 
Our results make clear that geographic researchers are aware of reproducible research practices, but lack direct experience using those practices.
Academic incentive systems and the inaccessibility of key components of prior research hinder reproducibility, and a small percentage of researchers are attempting to independently reproduce past work.  

Arising from the survey results, we see an opportunity for geographers to contribute to the interdisciplinary challenges and debates surrounding reproducibility.
There has been a tendency to reduce reproducibility to a matter of sharing computational artifacts (such as data and code), and to codify artifact sharing as the narrowed goal of reproducibility through requirements for publishing, funding, or badging.
While these practices may allow independent researchers to more easily reproduce and evaluate some aspects of prior studies, they lose sight of the underlying epistemological functions of reproduction studies.
Our results demonstrate that geographic researchers have a more varied understanding of reproducibility. 
While the discipline does not agree on the importance of sharing of artifacts for computational reproducibility, there is alignment on the clear, precise, and open communication of research and the use of reproduction studies to evaluate or extend the claims of prior work.
This shared understanding provides common ground for a discipline-wide debate about the role reproductions can or may play within different epistemologies and subfields, or in the presence of spatial heterogeneity and unique place-based characteristics. 

Our work also creates a foundation for the further empirical investigation of reproducibility within geography and its many disciplinary traditions, and more broadly across the sciences. 
We have made all the materials used in the development and execution of this research openly available so others can critique and extend our work.
We urge other researchers to reanalyze our data, replicate our study, improve our sampling frame and survey instrument, and progressively create a deeper understanding of questions we only begin to address in this work. 
One immediate path would be use our materials to survey geographic researchers about replicability, as our results show that some researchers appear to see a clearer role for replications over exact reproductions in their subfields, while other conflate reproducibility and replicability.
Disentangling these concepts and connecting them with the epistemological debates above is particularly salient in the context of convergence research addressing the most urgent challenges facing humanity, including climate change, global inequality and poverty, global health, and political conflict.


%%%%%%%%%%%%%%%%%%%%%%%%
\theendnotes


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgement(s)}
We thank Tyler Hoffman for providing technical assistance in the development and execution of a set of trial queries using the Scopus API.

\section*{Funding}
This material is based on work supported by the National Science Foundation under Grant No. \textbf{BCS-2049837}.

\section*{Notes on contributor(s)}
\textbf{Kedron:} Conceptualization, Methodology, Writing - Original Draft, Writing - Review and Editing, Supervision, Project Administration, Funding Acquisition. \textbf{Holler:} Conceptualization, Methodology, Data Curation, Writing - Review and Editing, Funding Acquisition. \textbf{Bardin:} Conceptualization, Methodology, Writing - Original Draft, Writing - Review and Editing, Data Curation, Software.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{landscape}
\begin{table}[h]
    \centering
    \begin{threeparttable}
    \caption{Descriptive summary of survey results}
    \begin{tabular}{l c c c c c c c c c c c c}
         \hline
                    & \multicolumn{4}{1}{Subfield}  & & \multicolumn{3}{1}{Approach} & &         &   & \\
         Measure    & PH & MT & NS & HU             & & QN & MX & QL                 & & Overall & N & Missing \\
         \hline
         Definition & \begin{tabular}[c]{@{}c@{}}1.73\\ (1.11)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}2.00\\ (1.09)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}1.86\\ (1.06)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}1.84\\ (1.14)\end{tabular} & 
                        & 
                        \begin{tabular}[c]{@{}c@{}}1.91\\ (1.13)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}1.82\\ (1.04)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}1.69\\ (1.26)\end{tabular} & 
                        & 
                        \begin{tabular}[c]{@{}c@{}}1.83\\ (1.12)\end{tabular} &
                        181 &
                        37 \\
         Familiarity &  \begin{tabular}[c]{@{}c@{}}3.71\\ (0.98)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}3.97\\ (0.86)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}2.82\\ (1.76)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}2.36\\ (1.36)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}3.75\\ (0.98)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}3.46\\ (1.22)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}1.75\\ (1.30)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}3.26\\ (1.36)\end{tabular} &
                        218 &
                        0 \\
         Experience & \begin{tabular}[c]{@{}c@{}}2.06\\ (1.18)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}1.85\\ (1.38)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}1.18\\ (0.96)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}0.60\\ (0.80)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}1.63\\ (1.32)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}1.69\\ (1.24)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}0.48\\ (0.68)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}1.44\\ (1.28)\end{tabular}
                        &
                        218 &
                        0 \\         
         Barriers & \begin{tabular}[c]{@{}c@{}}8.31\\ (3.00)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}9.31\\ (2.79)\end{tabular} & 
                        \begin{tabular}[c]{@{}c@{}}7.44\\ (3.20)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}7.32\\ (3.57)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}8.78\\ (2.80)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}8.58\\ (2.96)\end{tabular} &
                        \begin{tabular}[c]{@{}c@{}}5.97\\ (3.83)\end{tabular} &
                        &
                        \begin{tabular}[c]{@{}c@{}}8.20\\ (3.24)\end{tabular} &
                        182 &
                        36 \\
        \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item Acronyms indicate: \textit{PH} Physical Geography, \textit{MT} GIScience and Methods, \textit{NS} Nature and Society, \textit{HU} Human Geography; \textit{QN} Quantitative, \textit{MX} Mixed Methods, \textit{QL} Qualitative. 
    \end{tablenotes}
    \label{tab:overview}
    \end{threeparttable}
\end{table}
\end{landscape}

%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{landscape}
\begin{table}[h]
    \centering
    \begin{threeparttable}
    \caption{Barriers to Reproducibility}
    \begin{tabular}{l c c c c c c c c c c c}
         \hline
                    & \multicolumn{4}{1}{Subfield}  & & \multicolumn{3}{1}{Approach} & & & \\
         Barrier                            & PH & MT & NS & HU                   & & QN & MX & QL           & & Overall & Missing \\
         \hline
         \textit{Open Source Software}      & & & & & & & & & & & \\
         Familiarity with                   & 90.8\% & 100.0\% & 72.7\% & 71.7\% & & 96.7\% & 89.5\% & 52.5\% & & 85.3\% & 0 \\
         Use of                             & 47.4\% & 59.0\% & 31.8\% & 13.4\% & & 47.8\% & 41.1\% & 10.0\% & & 36.1\% & 0 \\
                                            & & & & & & & & & & &\\
         \textit{Lab and Field Notebooks}   & & & & & & & & & & & \\
         Familiarity with                   & 90.8\% & 78.0\% & 63.7\% & 35.8\% & & 75.0\% & 71.8\% & 40.0\% & & 67.0\% & 0 \\
         Use of                             & 63.0\% & 44.2\% & 40.9\% & 14.9\% & & 40.2\% & 39.4\% & 20.0\% & & 40.0\% & 2 \\
                                            & & & & & & & & & & & \\
         \textit{Sharing and Archiving Data}     & & & & & & & & & & & \\
         Familiarity with                   & 95.4\% & 95.4\% & 72.7\% & 71.7\% & & 93.5\% & 92.9\% & 57.5\% & & 86.7\% & 0 \\
         Use of                             & 63.0\% & 44.2\% & 40.9\% & 20.9\% & & 51.1\% & 51.8\% & 13.0\% & & 44.5\% & 0 \\
                                            & & & & & & & & & & & \\
         \textit{Sharing of Code/Scripts}     & & & & & & & & & & & \\
         Familiarity with                   & 64.6\% & 83.6\% & 45.4\% & 38.8\% & & 81.5\% & 57.7\% & 12.5\% & & 59.2\% & 0 \\
         Use of                             & 20.0\% & 32.8\% & 4.5\% & 10.5\% & & 22.8\% & 21.1\% & 5.0\% & & 18.8\% & 1 \\
                                            & & & & & & & & & & & \\
         \textit{Research Pre-registration}     & & & & & & & & & & & \\
         Familiarity with                   & 29.2\% & 27.2\% & 29.2\% & 17.9\% & & 28.3\% & 34.2\% & 12.5\% &  & 27.5\% & 0\\
         Use of                             & 4.6\% & 4.9\% & 0.0\% & 0.0\% & & 1.1\% & 5.9\% & 0.0\% &  & 2.7\% & 0 \\
                                            & & & & & & & & & & & \\
         N                                  & 65 & 61 & 22 & 67 & & 92 & 85 & 40 & & 218 & 0 \\
        \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item Cells report the percentage of respondents reporting being `somewhat' or `very' familiar with a reproducible research practice, or using those practices `most of the time' of `always'. Acronyms indicate: \textit{PH} Physical Geography, \textit{MT} GIScience and Methods, \textit{NS} Nature and Society, \textit{HU} Human Geography; \textit{QN} Quantitative, \textit{MX} Mixed Methods, \textit{QL} Qualitative. 
    \end{tablenotes}
    \label{tab:familar-use}
    \end{threeparttable}
\end{table}
\end{landscape}

%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{landscape}
\begin{table}[h]
    \centering
    \begin{threeparttable}
    \caption{Barriers to Reproducibility}
    \begin{tabular}{l c c c c c c c c c c c}
         \hline
                    & \multicolumn{4}{1}{Subfield}  & & \multicolumn{3}{1}{Approach} & & &\\
         Barrier    & PH & MT & NS & HU            & & QN & MX & QL              & & Overall & Missing\\
         \hline
         \textit{Research Environment}      & & & & & & & & & & &\\
         Pressure to Publish                & 83.1\% & 75.0\% & 54.5\% & 61.2\% & & 77.3\% & 75.3\% & 47.5\% & & 71.5\% & 11 \\
         Insufficient oversight             & 81.6\% & 82.0\% & 63.6\% & 56.7\% & & 82.6\% & 74.2\% & 40.0\% & & 71.1\% & 13 \\
         Fraud                              & 32.3\% & 39.3\% & 9.1\%  & 22.4\% & & 29.4\% & 31.8\% & 20.0\% & & 28.4\% & 13 \\
                                            & & & & & & & & & & &\\
         \textit{Artifact Availability}     & & & & & & & & & & &\\
         Insufficient metadata              & 76.9\% & 78.7\% & 59.1\% & 62.6\% & & 79.4\% & 72.9\% & 45.0\% & & 80.2\% & 15\\
         Unavailable Data                   & 89.3\% & 78.6\% & 59.1\% & 65.7\% & & 87.0\% & 81.2\% & 42.5\% & & 75.2\% & 15 \\
         Unavailable protocol/code          & 77.0\% & 86.9\% & 50.0\% & 59.7\% & & 87.0\% & 70.6\% & 37.5\% & & 71.1\% & 14 \\
         Published incomplete results       & 73.8\% & 78.7\% & 63.7\% & 56.7\% & & 78.2\% & 72.9\% & 37.2\% & & 68.4\% & 14 \\
         Use restricted data/software       & 56.9\% & 82.0\% & 40.9\% & 44.8\% & & 70.7\% & 56.5\% & 32.5\% & & 57.8\% & 19\\
                                            & & & & & & & & & & &\\
         \textit{Study Characteristics}     & & & & & & & & & & &\\
         Geographic variation               & 80.0\% & 72.2\% & 81.8\% & 61.2\% & & 79.4\% & 74.1\% & 40.0\% &  & 71.5\% & 16 \\
         Researcher positionality           & 50.8\% & 70.5\% & 72.8\% & 70.2\% & & 56.5\% & 65.9\% & 80.0\% &  & 64.2\% & 19\\
         Chance                             & 66.2\% & 65.6\% & 68.2\% & 55.2\% & & 67.4\% & 64.4\% & 52.5\% &  & 62.3\% & 19 \\
         Different computation              & 53.8\% & 68.9\% & 31.8\% & 38.8\% & & 63.1\% & 56.5\% & 25.0\% &  & 50.9\% & 18 \\
                                            & & & & & & & & & & &\\
         N                                  & 65 & 61 & 22 & 67 & & 92 & 85 & 40 & & 218 & 0 \\
        \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item Cells report the percentage of respondents reporting each factor occasionally or frequently contributed to a lack of reproducibility in geographic research. Acronyms indicate: \textit{PH} Physical Geography, \textit{MT} GIScience and Methods, \textit{NS} Nature and Society, \textit{HU} Human Geography; \textit{QN} Quantitative, \textit{MX} Mixed Methods, \textit{QL} Qualitative. 
    \end{tablenotes}
    \label{tab:barriers}
    \end{threeparttable}
\end{table}
\end{landscape}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\noindent PETER KEDRON is an Associate Professor in the School of Geographical Science and Urban Planning and core faculty member in the Spatial Analysis Research Center (SPARC) at Arizona State University, Tempe, AZ, 85283, US. Email: Peter.Kedron@asu.edu. His research program develops and uses spatial analytical methods to understanding persistently uneven patterns of spatial development. His recent work focuses on improving the production and accumulation of knowledge used to benefit society through policy. \\  
  
\noindent JOSEPH HOLLER is an Assistant Professor of Geography at Middlebury College. He is human geographer and geographic information scientist with research interests in social vulnerability and adaptation in the context of climate change and environmental degradation. His research intersects with work in political ecology, development geography, human dimensions of global change, and geographic information science. \\
  
\noindent SARAH BARDIN is a doctoral student in Geography at the School of Geographical Sciences and Urban Planning at Arizona State University. Her research focuses on leveraging spatial modeling and GIS to generate policy-relevant, actionable insights from data.

\end{document}
