\documentclass[]{interact}
\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.
\usepackage{subfigure}% Support for small, `sub' figures and tables
%\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required

\usepackage{natbib}
\bibliographystyle{chicago}
\setcitestyle{authoryear,open={(},close={)}}
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}% Bibliography support using natbib.sty

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=blue,
}

\usepackage{titlesec}
\titleformat*{\section}{\Large\bfseries}
\titleformat*{\subsection}{\large\bfseries}

\usepackage{endnotes}
\let\footnote=\endnote
\usepackage{etoolbox}
\patchcmd{\enoteformat}{1.8em}{0pt}{}{}

\theoremstyle{plain}% Theorem-like structures provided by amsthm.sty
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\articletype{DRAFT MANUSCRIPT}

\title{Reproducible Research Practices and Barriers to Reproducible Research in Geography: Insights from a Survey}

\author{
\name{Peter Kedron\textsuperscript{a,b}\thanks{CONTACT Peter Kedron. Email: Peter.Kedron@asu.edu}, Joseph Holler\textsuperscript{c}, and Sarah Bardin\textsuperscript{a,b}}
\affil{\textsuperscript{a}School of Geographical Sciences and Urban Planning, Arizona State University, Tempe, Arizona, USA; \textsuperscript{b}Spatial Analysis Research Center (SPARC), Arizona State University, Tempe, Arizona, USA; \textsuperscript{c}Department of Geography, Middlebury College, Middlebury, Vermont, USA}
}

\maketitle

\begin{abstract}
While the number of reproduction and replication studies undertaken in the social and behavioural sciences continues to rise, such studies have not yet become commonplace in geography. 
Existing attempts to reproduce geographic research suggest that many studies cannot be fully reproduced, or are simply missing components needed to attempt a reproduction. 
Despite this suggestive evidence, we have not yet systematically assessed geographers' perceptions of reproducibility, the use of reproducible research practices across the discipline's diverse research traditions, or identified the factors that have kept geographers from conducting more reproduction studies.
This study addresses each of these questions by surveying active geographic researchers selected using probability sampling techniques from a rigorously constructed sampling frame.
We find ...
CONClUSION...

\end{abstract}

\begin{keywords}
Reproducible Research, Epistemology, Geographic Research Methods
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Introduction}
Since the 1600s, replication has been a defining characteristic of the scientific method and an essential tool of researchers working to remove errors from our understanding of phenomena. 
\citet{nosek2020} broadly define a replication as any study that has at least one outcome that would be considered to be diagnostic evidence of a claim from prior research.
More frequently, replication is defined along two axes that help to distinguish the type of diagnostic evidence a study will provide and the function or purpose it is intended to serve. 
First, it is common to distinguish whether a replication study used the same data as the original study, or if new data were collected and analyzed. 
Second, it is helpful to identify whether a replication is focused on the question of whether the specific results of the original study can be reobserved, or whether the conclusions drawn from the original study are robust to changes in procedure or context.

When a researcher asks whether the same data and procedures can be used to generate the same results as an original study the central purpose of their study is verification.
If the researcher uses the original data, but introduces procedural differences they think may effect the original result they pursue a reanalysis designed to determine whether the original reasoning was somehow erroneous. 
Both of these approaches to replication assess the internal validity of research and are more commonly referred to as reproductions.
If the researcher tries to follow the procedures of an original study, but collects new data, the purpose shifts to evaluating the external validity of the original result by retesting it under new conditions.
This approach is commonly referred to as replication. 

While a replication or reproduction can never provide conclusive evidence for or against a finding, either type of study can be informative. 
If a well-executed, high-quality replication or reproduction recreates the result of an original study, we are apt to increase our confidence in the original findings. 
If a finding cannot be recreated, it reduces our confidence in the original result and suggests that our current understanding of the system being studied or our methods of testing that system are insufficient.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Prior Survey Research on Reproducible Research}
There are several ways to assess the reproducibility of research.
The most direct approach is to simply conduct a study that follows the methods of an existing study and then compare the results of the new study to the results of the old study. 
The direct approach can be implemented in two ways: as a one-to-one comparison, in which as single reproduction is used to assess the results and inferences of one prior study; or as a many-to-one comparison, in which teams of researchers attempt multiple reproductions and make a set of comparisons and collective assessment of one prior study \citep{NASEM2019}.
Published reproductions of both forms are becoming increasingly common in many disciplines with researchers generally only able to partially reproduce study results, often observing small effect sizes \citep[e.g.,][]{begley2012raise, camerer2016evaluating, camerer2018evaluating, wagenmakers2016registered}.  

Such published reproductions have not yet become common in geography. 
The small number of studies that have attempted assess the reproducibility of geographic research by reproducing existing work suggest that many studies cannot be fully reproduced \citep{Kedron2021ssrn, nust2018, ostermann2021}, or are simply missing components needed to attempt a reproduction \citep{Kedron_VijayanRP, konkol2019, ostermann2021}.
For example, as part of an ongoing research initiative of the Association of Geographic Information Laboratories in Europe, a group of researchers annually attempts to reproduce the computational results of submissions to that association's annual meeting \citep{Nust_AGILE_2020, Nust2021AGILE, Nust_AGILE_2022}. In published reports the researchers document whether the study could be reproduced and record factors that hindered reproduction such as the accessibility of data, code, and computational environment.
\citet{konkol2019} similarly attempted computational reproduction of 41 open-access research papers that used spatial methods.
Focusing their analysis on the execution of code and the similarity of figures, the authors found that 39 of the papers had coding issues that hindered reproduction and that 47 percent of the figures they reproduce deviated from the original result in a significant way. 


To date, the researchers attempting these reproductions have also emphasized the computational reproducibility of geographic research ahead of other dimensions. 


While these and similar efforts have produced useful guidelines \citep{hofer2019reproducible, wilson2021} and moved forward the development of the computational and institutional infrastructure \citep{nust2019, nust2021} needed to foster reproducibility in the field, they have not assessed the use of reproducible research practices across the disciplines diverse research traditions, or identified what has barred geographers from conducting more reproduction studies. 

The root of the problem is that the direct approach to assessing reproducibility presents a trade off. 
The careful reproduction of a study can provide rich evidence about that particular study, but offers at best limited evidence about the validity of other studies or the prevalence of the research practices observed.
For example, if a reproduction identified a flaw in the design of a study that impacted the results of that study it would be reasonable to be concerned that observing the same flaw in other studies could impact the results of those studies.
However, until other studies are gathered and checked we would know how common the flaw is, or whether or not it impacts other studies in a similar way. 

\subsection{Indirect}
Another approach to understanding the reproducibility of research is to survey active researchers about their perceptions of reproducibility and their own research practices.
Researcher surveys provide indirect measurements of reproducibility that offer  

\subsubsection{Geog Indirect}
For example, \citet{ostermann2017} use a literature review of volunteered geographic information research publications to assess computational reproducibility based on availability of original data, metadata, source code, or pseudocode.

\subsubsection{common problems of surveys}
While these surveys are suggestive of reproducibility concerns across the sciences, they share a set of common methodological problems. 
First, many prior surveys have used non-systematic sampling methods to draw data from non-representative, self-selected populations (e.g., professional association memberships), and/or use non-probability sampling schemes, which collectively limits our ability to draw generalizable conclusion or make inferences about specific sub-fields \citep{NASEM2019}.
Prior surveys also commonly failed to systematically report survey details (e.g., response rate) needed to assess and address potential bias. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Data and Methods}
Complete documentation of the procedures and materials used in this study are available through the Survey of Reproducibility in Geographic Research Repository (\citet{Kedron_Holler_Bardin_Hilgendorf_2022} - \url{https://osf.io/5yeq8/}) hosted by the Open Science Framework (OSF). 
Before the start of data collection, we registered a preanalysis plan for the survey with OSF Registries (\citet{Kedron_Survey_PAP} - \url{https://osf.io/6zjcp}). 
We amended that plan at the close of data collection to reflect a change in stopping rule we used to end the survey. 

The survey was conducted under the approval and supervision of the Arizona State Institutional Review Board - \textit{STUDY00014232}.
All approved documentation, study protocols, and consent materials are available through the above repository.

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Survey Design}
Our target population of interest is researchers who have recently published in the field of geography. 
We followed a 4-step procedure to create a sampling frame for our survey that captures this diverse population of researchers and the approaches they use when studying geography. 

First, beginning at the publication level, we identified journals indexed as either geography or physical geography by the \href{https://access.clarivate.com/}{Web of Science's Journal Citation Reports} that also had a 5-year impact factor greater than 1.5.
From those journals, we created a database of all articles published between 2017 and 2021.  

Second, we used Arizona State University's institutional subscription to the \href{https://www.scopus.com/home.uri}{Scopus Database} to extract journal information (e.g., subject area, ranking), article information (e.g., abstract, citation counts), and author information (e.g., corresponding status, email) for each publication. 
Because our intention is to capture individuals actively publishing new geographic research, we retained publications indexed by Scopus as \textit{document type = "Article"} and removed all other publication types (e.g., editorials, book reviews) from our article database. 
We also removed articles with missing authorship information. 

Third, we next moved to the author level to create a condensed list of corresponding authors. 
We chose to focus on corresponding authors for two reasons. 
(1) Corresponding authorship is one indicator of the level of involvement an individual had in a given work. 
While imperfect, it was the best available indicator in the Scopus database as across journals there is no commonly adopted policy for declarations of author work (e.g., CRediT Statements).
(2) Scopus maintains email contact information for all corresponding authors, which gave us a means of contacting researchers in our sampling frame.
Scopus also maintains a unique identifier for each author (author-id) across time, which allowed us to identify authors across publications. 

Fourth, we associated each corresponding author with their contact email address and deduplicated to create a single record for each corresponding author. 
To maximize our chance or response, we retained the latest available contact information for each author by sorting the initial list by author-id and publication year (descending) and keeping the latest available entry for each author-id. 
For authors who had two or more distinct emails in the latest year of publication, we deduplicated by giving a ranked preference to .edu, .gov, and then .org extensions.

Applying these criteria yielded a sampling frame of 29,828 authors. 
On average, these authors published 2.7 articles in geography journals meeting our criteria between 2017 and 2021. 
Roughly one-third (33 percent) were most recently a corresponding author for an article published in a general geography journal. 
A similar proportion (32 percent) were most recently a corresponding author for an article published in an earth sciences journal, and smaller proportions in the social sciences and cultural geography (20 percent and 16 percent, respectively).

The survey instrument used in this study is available through the Survey of Reproducibility in Geographic Research Repository.
The survey consists of 23 questions that assess (i) perceptions of the reproducibility of geographic research, (ii) familiarity and use of reproducible research practices, and (iii) beliefs about barriers to reprodicibility. 
Survey questions were developed following a review of prior reproducibility surveys \citep[e.g.,][]{fanelli2009many,baker20161, konkol2019} and our own reading of recurring issues in the reproducibility literature. 
Because this survey had not been previously fielded, we pilot tested the instrument with a subset of \textit{n}=19 graduate students and geography faculty with differing levels of experience, topical focus, and methodological background. 
After pilot testing we removed these individuals from our sampling frame to ensure they would not be included in our final sample.

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Data Collection}
We used a digital form of the Tailored Design Method \citep{dillman2014internet} to survey geographic researchers between May 17 and June 10, 2022.
A simple random sample of 2,000 researchers was drawn without replacement from our sampling frame, and those researchers were invited via email to participate in the online survey. 
Researchers received their initial invitation on May 17, 2022. 
Two reminder emails were sent to researchers that had not yet completed the survey on May 26 and May 31, 2022.

Participation in the survey was entirely voluntary. 
Each researcher that opted to participate in the survey was provided with IRB approved consent documentation and linked to the internet survey instrument. 
Participants were also given the option to enter a random prize draw. 
The total possible compensation a participant was eligible to receive was 90 US dollars, awarded as a prepaid credit card.
Three prize winners were selected at random from those electing to enter the prize draw at the end of the data collection period.

The online survey was administered through \href{https://www.qualtrics.com/}{Qualtrics}. 
Each question on the survey was presented as a unique page. 
Adaptive questioning, conditionally displaying items based on responses to other items, was used to reduce the number of complexity of questions.
Participating researchers had the option to exit and re-enter the survey and were also able to review and/or change their answers using a back button as they progressed through the survey.

At the end of the data collection period, responses were checked for completeness and coded using the reporting standards of the American Association For Public Opinion Research \citep{aaporstandards}.
Responses were downloaded from Qualtrics, anonymized, and stored in a password-protected databases.
We used a unique key to link this data with participant information (e.g., citation history) stored in a separate database during our statistical analyses. 
To preserve participant privacy that connection was severed at the end of our analysis.
We have only shared the anonymized response file through our public repository. 
As a result some of the result presented below cannot be directly replicated. 
We have indicated throughout our results section which analysis can and cannot be replicated using the data files and code share through our public repository. 

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Statistical Analyses}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Results}

A total of \textit{n}=215 of the authors we contacted completed the online survey with information sufficient for analysis. The contact rate for the survey was 13.9 percent, the response rate was 10.8 percent, yielding a cooperation rate of 77.6 percent. The refusal rate was 3.1 percent.\endnote{All outcome rates are reported using \citet{aaporstandards} standards. The outcome rates used were - response rate 2, cooperation rate 2, refusal rate 1, and contact rate 1.}   


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Discussion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Conclusion}
A standing question is how should geographic research approaches be designed to efficiently generate reliable knowledge.

\theendnotes


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgement(s)}
We thank Tyler Hoffman for providing technical assistance in the development and execution of a set of trial queries using the Scopus API.

\section*{Funding}
This material is based on work supported by the National Science Foundation under Grant No. \textbf{BCS-2049837}.

\section*{Notes on contributor(s)}
\textbf{Kedron:} Conceptualization, Methodology, Writing - Original Draft, Writing - Review and Editing, Supervision, Project Administration, Funding Acquisition. \textbf{Holler:} Conceptualization, Methodology, Data Curation, Writing - Review and Editing, Funding Acquisition. \textbf{Bardin:} Conceptualization, Methodology, Writing - Original Draft, Writing - Review and Editing, Data Curation, Software.



\newpage
\bibliography{references}

\newpage
\noindent PETER KEDRON is an Associate Professor in the School of Geographical Science and Urban Planning and core faculty member in the Spatial Analysis Research Center (SPARC) at Arizona State University, Tempe, AZ, 85283, US. Email: Peter.Kedron@asu.edu. His research interests include spatial analysis, geographic information science, economic geography, and the accumulation of knowledge about geographic phenomena. \\  
  
\noindent JOSEPH HOLLER is an Assistant Professor of Geography at Middlebury College, Middlebury, VT, 05753, US. Email: \\
  
\noindent SARAH BARDIN is a PhD candidate ...

\end{document}
